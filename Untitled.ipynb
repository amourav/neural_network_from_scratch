{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_derivative(y_true, y_pred):\n",
    "    # dE / da\n",
    "    # input: N x K\n",
    "    # output: N x K array\n",
    "    N = len(y_true)\n",
    "    return -(y_true / y_pred) / N\n",
    "\n",
    "\n",
    "def one_hot_encode(y, n_classes):\n",
    "    y_onehot = np.zeros((len(y), n_classes))\n",
    "    for i, y_i in enumerate(y):\n",
    "        y_onehot[i, y_i] = 1\n",
    "    return y_onehot\n",
    "\n",
    "def softmax(x):\n",
    "    # activation (a)\n",
    "    # input: N x K array\n",
    "    # output: N x K array\n",
    "    # https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "    exp = np.exp(x - np.max(x))\n",
    "    return exp / np.sum(exp, axis=1)[:, None]\n",
    "\n",
    "def softmax_derivative(Z):\n",
    "    # da/dz\n",
    "    #input: N x K array\n",
    "    #output: N x K x K array\n",
    "    #http://saitcelebi.com/tut/output/part2.html\n",
    "    N, K = Z.shape\n",
    "    s = softmax(Z)[:, :, np.newaxis]\n",
    "    a = np.tensordot(s, np.ones((1, K)), axes=([-1],[0]))\n",
    "    I = np.repeat(np.eye(K, K)[np.newaxis, :, :], N, axis=0)\n",
    "    b = I - np.tensordot(np.ones((K, 1)), s.T, axes=([-1],[0])).T\n",
    "    return a * np.swapaxes(b, 1, 2)\n",
    "\n",
    "def softmax_derivative_test(Z):\n",
    "    # da/dz\n",
    "    # non-vectorized softmax gradient calculation\n",
    "    #http://saitcelebi.com/tut/output/part2.html\n",
    "    N, K = Z.shape\n",
    "    da_dz = np.zeros((N, K, K))\n",
    "    kron_delta = np.eye(K)\n",
    "    s = softmax(Z)\n",
    "    for n in range(N):\n",
    "        for i in range(K):\n",
    "            for j in range(K):\n",
    "                da_dz[n, i, j] = s[n, i] * (kron_delta[i, j] - s[n, j])\n",
    "    return da_dz\n",
    "\n",
    "\n",
    "def dE_dz_test2(dE_da, da_dz):\n",
    "    # array (N x K)\n",
    "    # array (N x K x K)\n",
    "    # output: array (N x K)\n",
    "    N, K = dE_da.shape\n",
    "    dE_dz = np.zeros((N, K))\n",
    "    for n in range(N):\n",
    "        dE_dz[n, :] = np.matmul(da_dz[n], dE_da[n, :, np.newaxis]).T\n",
    "    return dE_dz\n",
    "\n",
    "def some_type_of_matrix_multiplication_(dE_da, da_dz):\n",
    "    # how do i get dE/dz from dE_da and da_dz\n",
    "    pass\n",
    "\n",
    "X = np.random.rand(100, 2)\n",
    "W = np.random.rand(2, 4)\n",
    "y = np.random.randint(0, 4, size=100)\n",
    "y = one_hot_encode(y, 4)\n",
    "Z = X @ W\n",
    "S = softmax(Z)\n",
    "N, K = Z.shape\n",
    "\n",
    "# da / dz for softmax\n",
    "da_dz = softmax_derivative(Z) # (100, 4, 4)\n",
    "da_dz_test = softmax_derivative_test(Z) # (100, 4, 4) - non vectorized implementation\n",
    "print(np.isclose(da_dz, da_dz_test).all()) # equivalence test\n",
    "\n",
    "dE_da = cross_entropy_derivative(y, S) # (100, 4)\n",
    "dE_dz = some_type_of_matrix_multiplication_(dE_da, da_dz) # what do I do here? *****\n",
    "dE_dz_test  = (S - y) / N # (100, 4) If you combine dE/da and da/dz terms\n",
    "dE_dz_test2 = dE_dz_test2(dE_da, da_dz)\n",
    "print(np.isclose(dE_dz_test, dE_dz_test2).all()) # equivalence test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
