{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_norm_layer_init(input_shape, output_shape):\n",
    "    return np.random.normal(size=(input_shape, output_shape))\n",
    "\n",
    "def ones_layer_init(input_shape, output_shape):\n",
    "    return np.ones((input_shape, output_shape))\n",
    "\n",
    "def zeros_init_layer(length):\n",
    "    return np.zeros(length)\n",
    "\n",
    "def softmax(x):\n",
    "    # https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "    exp = np.exp(x - np.max(x))\n",
    "    return exp / np.sum(exp, axis=1)[:, None]\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(X):\n",
    "    return np.ones((X.shape[0]))\n",
    "\n",
    "def softmax_grad(x):\n",
    "    s = sigmoid(x)\n",
    "    _, K = s.shape\n",
    "    return s @ (np.eye(K, K) - s).T\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(x, 0, x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x > 0).astype(int)\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "    return -np.sum(y_true*np.log(y_pred)) / N\n",
    "\n",
    "def cross_entropy_derivative(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "    return -y_true/y_pred / N\n",
    "\n",
    "def onehot_encode(y):\n",
    "    y = np.array(y)\n",
    "    y_onehot = np.zeros((len(y), max(y)+1))\n",
    "    for i, y_i in enumerate(y):\n",
    "        y_onehot[i, y_i] = 1\n",
    "    return y_onehot\n",
    "\n",
    "def one_hot_encode(y, n_classes):\n",
    "    y_onehot = np.zeros((len(y), n_classes))\n",
    "    for i, y_i in enumerate(y):\n",
    "        y_onehot[i, y_i] = 1\n",
    "    return y_onehot\n",
    "'''\n",
    "def softmax_grad(x):\n",
    "    # da / dZ\n",
    "    # Reshape the 1-d softmax to 2-d so that np.dot will do the matrix multiplication\n",
    "    s = softmax(x).reshape(-1,1)\n",
    "    return np.diagflat(s) - np.dot(s, s.T)\n",
    "'''\n",
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hidden=(8, 6),\n",
    "                 init_weights='unit_norm',\n",
    "                 init_bias='zeros',\n",
    "                 activation='sigmoid',\n",
    "                 loss='cross_entropy',\n",
    "                 mode='classification',\n",
    "                 random_state=1):\n",
    "        self.hidden = hidden\n",
    "        self.init_weights = init_weights\n",
    "        self.init_bias = init_bias\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.random_state = random_state\n",
    "        self.mode = mode\n",
    "        np.random.seed(self.random_state)\n",
    "        self._set_act_func()\n",
    "        self._set_loss()\n",
    "        \n",
    "        \n",
    "    def _init_neural_network(self):\n",
    "        implemented_weight_inits = {'unit_norm': unit_norm_layer_init,\n",
    "                                    'ones': ones_layer_init\n",
    "                                   }\n",
    "        implemented_bias_inits = {'zeros': zeros_init_layer,\n",
    "                                   }\n",
    "        try:\n",
    "            init_layer_weight = implemented_weight_inits[self.init_weights]\n",
    "            init_layer_bias = implemented_bias_inits[self.init_bias]\n",
    "        except KeyError:\n",
    "            raise Exception('{} or {} not accepted'.format(self.init_weights,\n",
    "                                                           self.init_bias))\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for layer in range(len(self.hidden) + 1):\n",
    "            if layer == 0:\n",
    "                input_shape = self.n_features\n",
    "                output_shape = self.hidden[layer]\n",
    "            elif layer == len(self.hidden):\n",
    "                input_shape = self.hidden[layer - 1]\n",
    "                output_shape = self.n_classes\n",
    "            else:\n",
    "                input_shape = self.hidden[layer - 1]\n",
    "                output_shape = self.hidden[layer]                \n",
    "            w_l = init_layer_weight(input_shape, output_shape)\n",
    "            b_l = init_layer_bias(output_shape)\n",
    "            self.weights.append(w_l)\n",
    "            self.biases.append(b_l)\n",
    "        \n",
    "            \n",
    "    def _set_act_func(self):\n",
    "        implemented_activations = {'sigmoid': sigmoid,\n",
    "                                   'ReLU': ReLU,\n",
    "                                   'linear': linear_derivative}\n",
    "        # set activation function\n",
    "        try:\n",
    "            self.act = implemented_activations[self.activation]\n",
    "        except KeyError:\n",
    "            raise Exception('{} not accepted'.format(self.activation))\n",
    "            \n",
    "        implemented_derivatives = {'sigmoid': sigmoid_derivative,\n",
    "                                   'ReLU': ReLU_derivative,\n",
    "                                   'linear': linear_derivative}\n",
    "        \n",
    "        # set activation derivative (da/dz)\n",
    "        try:\n",
    "            self.act_derivative = implemented_derivatives[self.activation]\n",
    "        except KeyError:\n",
    "            raise Exception('derivative not implemented for {}'.format(self.activation))\n",
    "\n",
    "        # set activation for last layer (softmax for classification and linear for regression)\n",
    "        if self.mode == 'classification':\n",
    "            self.last_act = softmax\n",
    "        elif self.mode == 'regression':\n",
    "            self.last_act = linear\n",
    "\n",
    "    def _set_loss(self):\n",
    "        implemented_losses = {'cross_entropy': cross_entropy,}\n",
    "        try:\n",
    "            self.loss_func = implemented_losses[self.loss]\n",
    "        except KeyError:\n",
    "            raise Exception('{} not accepted'.format(self.loss))\n",
    "    \n",
    "    def train(self, X, y, n_epochs=10, lr=0.001, n_classes=None):\n",
    "        self.lr = lr\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.classes = n_classes\n",
    "        if n_classes is None:\n",
    "            self.classes = set(y)\n",
    "            self.n_classes = len(self.classes)\n",
    "        \n",
    "        y_one_hot = one_hot_encode(y, self.n_classes)\n",
    "        self._init_neural_network()\n",
    "        \n",
    "        for e in range(n_epochs):\n",
    "            # implement shuffle\n",
    "            # implement batch\n",
    "            self._feed_forward(X)\n",
    "            self.loss_e = self.loss_func(y_one_hot, self.activations[-1])\n",
    "            print('loss', e, self.loss_e)\n",
    "            self._back_prop(X, y_one_hot)\n",
    "\n",
    "    \n",
    "    def _feed_forward(self, X):\n",
    "        self.activations = []\n",
    "        self.Z_list = []\n",
    "        #self.Z_list.append(X)\n",
    "        act = self.act\n",
    "        for layer, (w_l, b_l) in enumerate(zip(self.weights, self.biases)):\n",
    "            if layer == 0:\n",
    "                prev = X\n",
    "            else:\n",
    "                prev = self.activations[-1]\n",
    "\n",
    "            if layer == len(self.hidden):\n",
    "                act = self.last_act\n",
    "            Z_l = np.dot(prev, w_l) + b_l\n",
    "            act_l = act(Z_l)    \n",
    "            self.activations.append(act_l)\n",
    "            self.Z_list.append(Z_l)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        self._feed_forward(X)\n",
    "        return self.activations[-1]\n",
    "            \n",
    "    def _dE_dZ(self, y, p):\n",
    "        # dE/dz where E(y) - cross entropy and a(z) is the softmax activation function\n",
    "        return p - y\n",
    "            \n",
    "    def _back_prop(self, X, y):\n",
    "        y_pred = self.activations[-1]\n",
    "        \n",
    "        # self.dE_dp = cross_entropy_derivative(y, y_pred)\n",
    "        new_weights, new_biases = [], []\n",
    "        L = len(self.activations)\n",
    "        for layer in range(L-1, -1, -1):\n",
    "            w_l, b_l = self.weights[layer], self.biases[layer]\n",
    "            Z_l = self.Z_list[layer]\n",
    "            \n",
    "            if layer == 0:\n",
    "                act_prev = X\n",
    "            else:\n",
    "                act_prev = self.activations[layer-1]\n",
    "            \n",
    "            if layer == L-1:\n",
    "                self.dE_dz = self._dE_dZ(y, y_pred)\n",
    "            else:\n",
    "                dE_da = self.dE_dz @ self.weights[layer+1].T # dE_da wrt activation of current layer\n",
    "                da_dz = self.act_derivative(Z_l)\n",
    "                self.dE_dz = np.multiply(da_dz, dE_da)\n",
    "            \n",
    "            dE_dW = act_prev.T @ self.dE_dz\n",
    "            dE_db = np.sum(self.dE_dz, axis=0)\n",
    "            print(layer, act_prev.T.shape, self.dE_dz.shape, dE_dW.shape, w_l.shape)\n",
    "            w_l -= self.lr * dE_dW\n",
    "            b_l -= self.lr * dE_db\n",
    "            \n",
    "            new_weights.append(w_l)\n",
    "            new_biases.append(b_l)\n",
    "            \n",
    "        self.weights = new_weights[::-1]\n",
    "        self.biases = new_biases[::-1]\n",
    "            #Z_l = self.Z_list[layer]\n",
    "            #self.act_l = self.activations[layer]\n",
    "            #prev_act = self.activations[layer-1]\n",
    "            \n",
    "            \n",
    "            #self.da_dz = softmax_grad(Z_l)\n",
    "            #if layer == 0: # last layer\n",
    "            #    dE_dZ_l = dE_dyp # N vec X N vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris Dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  \n",
    "y = iris.target\n",
    "\n",
    "# For illustration purposes we will only be using the two features in the dataset\n",
    "feature_idxs = [1, 3] # SET FEATURES BY INDEX <------------------\n",
    "\n",
    "feature_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "xlbl, ylbl = feature_names[feature_idxs[0]], feature_names[feature_idxs[1]] \n",
    "# We will also split the dataset into training and testing so we can evaluate the kNN classifier\n",
    "X_trn_, X_test_, y_trn, y_test = train_test_split(X, \n",
    "                                                 y, \n",
    "                                                 test_size=0.333, \n",
    "                                                 random_state=0,\n",
    "                                                 stratify=y)\n",
    "X_trn, X_test = X_trn_[:, feature_idxs], X_test_[:, feature_idxs]\n",
    "\n",
    "print(\"X_trn.shape = {}, X_test.shape = {}\".format(X_trn.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "nn = NeuralNetwork(hidden=(8,6), \n",
    "                   init_weights='unit_norm', \n",
    "                   activation='ReLU',\n",
    "                   )\n",
    "nn.train(X_trn, y_trn, n_epochs=100, lr=0.0001)\n",
    "y_pred_trn = nn.predict(X_trn).argmax(axis=1)\n",
    "y_pred_test = nn.predict(X_test).argmax(axis=1)\n",
    "print(accuracy_score(y_pred_trn, y_trn))\n",
    "print(accuracy_score(y_pred_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0 1.3732304002112679\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 1 0.7325920254029691\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 2 0.7117990177688739\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 3 0.7050309385087026\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 4 0.7026680684655983\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 5 0.7010232938654418\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 6 0.6998592260929809\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 7 0.6988646999495286\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 8 0.6979784197436626\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 9 0.6971736834492765\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 10 0.696437075253671\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 11 0.6957609424331621\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 12 0.6951387692057622\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 13 0.6945653060668701\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 14 0.6940365674977231\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 15 0.69354474741344\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 16 0.6930653361776232\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 17 0.6925494609612176\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 18 0.692092821529281\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 19 0.6916647088620994\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 20 0.6912626859012448\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 21 0.6908806283549572\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 22 0.6905168628143562\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 23 0.690143708537593\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 24 0.6897199311131391\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 25 0.6892882708059597\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 26 0.6888319227188717\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 27 0.6883996910722893\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 28 0.6879852612332752\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 29 0.6875408723819393\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 30 0.6869347288199751\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 31 0.6864665256314759\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 32 0.6860174661343527\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 33 0.6855488488398184\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 34 0.6851010288074785\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 35 0.6847455684847285\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 36 0.6844430891992814\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 37 0.6841997421466308\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 38 0.6839615187724118\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 39 0.6837363034098729\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 40 0.6835872897625219\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 41 0.6834413173055945\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 42 0.6832982798601003\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 43 0.6831536827394473\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 44 0.6829279913101303\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 45 0.6827185486905278\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 46 0.6825761621253045\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 47 0.6824411737331317\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 48 0.682308266953925\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 49 0.6821778596988513\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 50 0.6820503622109765\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 51 0.681926541951693\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 52 0.6818044440333462\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 53 0.6816843229827864\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 54 0.6815666301417409\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 55 0.6814384542978715\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 56 0.6813397647672719\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 57 0.6812074011209637\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 58 0.6811211404248398\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 59 0.6810459401664645\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 60 0.6809738784260253\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 61 0.6809037455109366\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 62 0.6808368556534927\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 63 0.6807723874076337\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 64 0.6807093669471896\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 65 0.680647702522369\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 66 0.6805873190732896\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 67 0.6805281500546292\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 68 0.6804701357240145\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 69 0.6804016140319331\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 70 0.6803138283454412\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 71 0.6802377936005501\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 72 0.6801728677697685\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 73 0.6801106639656447\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 74 0.6800500520477946\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 75 0.6799908420197142\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 76 0.6799329629781354\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 77 0.6798763628167955\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 78 0.6798209940948264\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 79 0.6797668124505016\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 80 0.6797297421237833\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 81 0.6796935526563656\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 82 0.6796647980311028\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 83 0.679612176245715\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 84 0.6795944218498859\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 85 0.6795361542688819\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 86 0.6794867791428421\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 87 0.6794487074772572\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 88 0.6794113734336455\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 89 0.6793948288278514\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 90 0.6793446970816029\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 91 0.6793068774653939\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 92 0.679271021055341\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 93 0.6792368153668565\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 94 0.6792215735028256\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 95 0.6791736853146607\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 96 0.679137965637477\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 97 0.6791041704569207\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 98 0.679072516548199\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n",
      "loss 99 0.6790587541004897\n",
      "2 (6, 100) (100, 2) (6, 2) (6, 2)\n",
      "1 (8, 100) (100, 6) (8, 6) (8, 6)\n",
      "0 (2, 100) (100, 8) (2, 8) (2, 8)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(100, 2)\n",
    "y = np.random.randint(0, 2, size=100)\n",
    "\n",
    "nn = NeuralNetwork(hidden=(8,6), \n",
    "                   init_weights='unit_norm', \n",
    "                   activation='ReLU',\n",
    "                   )\n",
    "nn.train(X, y, n_epochs=100, lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'a']"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['a', 'b'][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "L = 3\n",
    "for i in range(L-1, -1, -1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(100, 2)\n",
    "y = np.random.randint(0, 2, size=100)\n",
    "\n",
    "nn = NeuralNetwork(hidden=(8,6), init_weights='unit_norm', activation='ReLU')\n",
    "nn.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.6941128459836579\n",
      "2 (6, 3) (3, 2) (6, 2) (6, 2)\n",
      "1 (8, 3) (3, 6) (8, 6) (8, 6)\n",
      "0 (2, 3) (3, 8) (2, 8) (2, 8)\n",
      "loss 0.6942956806651174\n",
      "2 (6, 3) (3, 2) (6, 2) (6, 2)\n",
      "1 (8, 3) (3, 6) (8, 6) (8, 6)\n",
      "0 (2, 3) (3, 8) (2, 8) (2, 8)\n",
      "loss 0.694474433866695\n",
      "2 (6, 3) (3, 2) (6, 2) (6, 2)\n",
      "1 (8, 3) (3, 6) (8, 6) (8, 6)\n",
      "0 (2, 3) (3, 8) (2, 8) (2, 8)\n",
      "loss 0.694653732909699\n",
      "2 (6, 3) (3, 2) (6, 2) (6, 2)\n",
      "1 (8, 3) (3, 6) (8, 6) (8, 6)\n",
      "0 (2, 3) (3, 8) (2, 8) (2, 8)\n",
      "loss 0.694833579551987\n",
      "2 (6, 3) (3, 2) (6, 2) (6, 2)\n",
      "1 (8, 3) (3, 6) (8, 6) (8, 6)\n",
      "0 (2, 3) (3, 8) (2, 8) (2, 8)\n",
      "loss 0.6950139755569036\n",
      "2 (6, 3) (3, 2) (6, 2) (6, 2)\n",
      "1 (8, 3) (3, 6) (8, 6) (8, 6)\n",
      "0 (2, 3) (3, 8) (2, 8) (2, 8)\n",
      "loss 0.6951949226932953\n",
      "2 (6, 3) (3, 2) (6, 2) (6, 2)\n",
      "1 (8, 3) (3, 6) (8, 6) (8, 6)\n",
      "0 (2, 3) (3, 8) (2, 8) (2, 8)\n",
      "loss 0.6953764227355276\n",
      "2 (6, 3) (3, 2) (6, 2) (6, 2)\n",
      "1 (8, 3) (3, 6) (8, 6) (8, 6)\n",
      "0 (2, 3) (3, 8) (2, 8) (2, 8)\n",
      "loss 0.6955584774635007\n",
      "2 (6, 3) (3, 2) (6, 2) (6, 2)\n",
      "1 (8, 3) (3, 6) (8, 6) (8, 6)\n",
      "0 (2, 3) (3, 8) (2, 8) (2, 8)\n",
      "loss 0.6957410886626646\n",
      "2 (6, 3) (3, 2) (6, 2) (6, 2)\n",
      "1 (8, 3) (3, 6) (8, 6) (8, 6)\n",
      "0 (2, 3) (3, 8) (2, 8) (2, 8)\n",
      "0.6957410886626646\n",
      "0 [[ 0.1625419  -0.06117564 -0.05289767 -0.10729686  0.08638613 -0.23015387\n",
      "   0.17456339 -0.07612069]\n",
      " [ 0.03213543 -0.02493704  0.14603971 -0.20601407 -0.03239635 -0.03840544\n",
      "   0.11354795 -0.10998913]] - [ 1.11110488e-04  0.00000000e+00 -1.71088281e-04  0.00000000e+00\n",
      " -1.54632365e-04  0.00000000e+00  7.29089752e-05  0.00000000e+00] - [[1.94744479e-01 0.00000000e+00 9.30132798e-02 0.00000000e+00\n",
      "  5.38816106e-02 0.00000000e+00 2.88152424e-01 0.00000000e+00]\n",
      " [1.01213107e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 6.65896121e-05 0.00000000e+00]\n",
      " [3.22133699e-02 0.00000000e+00 1.45902890e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.13597314e-01 0.00000000e+00]]\n",
      "1 [[-0.01724282 -0.08779208  0.00422137  0.05817477 -0.11006192  0.11473872]\n",
      " [ 0.09015907  0.05024943  0.09008559 -0.06837279 -0.01228902 -0.09357694]\n",
      " [-0.02678881  0.05300713 -0.06916608 -0.03978792 -0.06871727 -0.08423964]\n",
      " [-0.06712461 -0.00126646 -0.11173103  0.02344157  0.16598022  0.07420442]\n",
      " [-0.01918356 -0.0887629  -0.07471583  0.16921997  0.00508078 -0.06363595]\n",
      " [ 0.01909155  0.21002551  0.0120159   0.06172031  0.03001703 -0.03522498]\n",
      " [-0.11425182 -0.03495635 -0.02088942  0.05847325  0.08389834  0.09358201]\n",
      " [ 0.02855873  0.08851412 -0.07543979  0.12528682  0.05129298 -0.02980928]] - [ 0.         -0.00019454  0.         -0.00094126  0.          0.00129864] - [[0.         0.         0.         0.03275712 0.         0.03920413]\n",
      " [0.         0.         0.         0.         0.         0.00119738]\n",
      " [0.         0.00076051 0.         0.00186831 0.         0.00320517]]\n",
      "2 [[ 0.04885181 -0.00755717]\n",
      " [ 0.11316733  0.15197729]\n",
      " [ 0.21855754 -0.13964963]\n",
      " [-0.1442329  -0.05062506]\n",
      " [ 0.01600371  0.08761689]\n",
      " [ 0.03176668 -0.2024233 ]] - [ 0.00504998 -0.00504998] - [[0.50379867 0.49620133]\n",
      " [0.50234079 0.49765921]\n",
      " [0.5024072  0.4975928 ]]\n",
      "(2, 8) - (8,) - (3, 8) - (3, 8)\n",
      "(8, 6) - (6,) - (3, 6) - (3, 6)\n",
      "(6, 2) - (2,) - (3, 2) - (3, 2)\n",
      "(3, 8)\n"
     ]
    }
   ],
   "source": [
    "X = [[1, 1], [0, 0], [0, 1]]\n",
    "y = [1, 0, 1]\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "nn = NeuralNetwork(hidden=(8,6), init_weights='unit_norm', activation='ReLU')\n",
    "nn.train(X, y)\n",
    "print(nn.loss_e)\n",
    "for i in range(len(nn.activations)):\n",
    "    \n",
    "    print(i, nn.weights[i], '-', nn.biases[i],'-', nn.activations[i])\n",
    "    \n",
    "for i in range(len(nn.activations)):\n",
    "    print(nn.weights[i].shape, '-', nn.biases[i].shape,'-', nn.activations[i].shape, '-', nn.Z_list[i].shape)\n",
    "\n",
    "print(nn.dE_dz.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = np.sum(nn.dE_dz, axis=0)\n",
    "np.ones(nn.dE_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.random.rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,3) (4,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-5d23293c344d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msoftmax_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-60-b55d5dec4194>\u001b[0m in \u001b[0;36msoftmax_grad\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m@\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,3) (4,3) "
     ]
    }
   ],
   "source": [
    "softmax_grad(np.array([[0.2, 0.1, 0.7], [0.2, 0.5, 0.3], [0.2, 0.6, 0.2], [0.7, 0.1, 0.2]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28140804, 0.25462853, 0.46396343],\n",
       "       [0.28943311, 0.39069383, 0.31987306],\n",
       "       [0.28638322, 0.42723356, 0.28638322],\n",
       "       [0.46396343, 0.25462853, 0.28140804]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = softmax(np.array([[0.2, 0.1, 0.7], [0.2, 0.5, 0.3], [0.2, 0.6, 0.2], [0.7, 0.1, 0.2]]))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,3,3) (4,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-2619dde0ebf2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mI\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,3,3) (4,3) "
     ]
    }
   ],
   "source": [
    "N, K = s.shape\n",
    "I = np.repeat(np.eye(K, K)[np.newaxis, :, :], N, axis=0)\n",
    "(I - s @ np.eye(K, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.multiply.outer(s, np.eye(1, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2 = s[0, : , np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.20221756, -0.07165452, -0.13056304],\n",
       "       [-0.07165452,  0.18979284, -0.11813832],\n",
       "       [-0.13056304, -0.11813832,  0.24870137]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://saitcelebi.com/tut/output/part2.html\n",
    "\n",
    "s0 = s[0, : , np.newaxis]\n",
    "tmp = np.identity(K) - np.matmul(np.ones((K, 1)), s0.T)\n",
    "tmp2 = np.matmul(s0, np.ones((1, K)))\n",
    "tmp3 = tmp2 * tmp\n",
    "tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(np.ones((K, 1)), s0.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 1), (1, 3), (3, 3))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((K, 1)).shape, s0.T.shape, np.matmul(np.ones((K, 1)), s0.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3, 1) (1, 3, 4)\n",
      "(4, 3, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "I2 = np.ones((N, K))[:, : , np.newaxis]\n",
    "s2 = s[:, : , np.newaxis]\n",
    "\n",
    "print(I2.shape, s2.T.shape), \n",
    "print(np.tensordot(I2, s2.T, axes=([2],[0])).shape) #np.matmul(I2, s2.T).shape np.tensordot(I2, s2.T, axes=([2,1],[0,1])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,3,3) (4,3,4) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-157-a36dc7abf298>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ms2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mI\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtmp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtmp3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtmp3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,3,3) (4,3,4) "
     ]
    }
   ],
   "source": [
    "tmp = I - np.matmul(I, s2.T)\n",
    "tmp2 = np.matmul(s2, np.ones((1, K)))\n",
    "tmp3 = tmp2 * tmp\n",
    "tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3, 4)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(I, s2.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.eye(2, 2)\n",
    "print(a.shape)\n",
    "# (2,  2)\n",
    "\n",
    "# indexing with np.newaxis inserts a new 3rd dimension, which we then repeat the\n",
    "# array along, (you can achieve the same effect by indexing with None, see below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0, 1, 2, 1, 0]\n",
    "y_ = np.zeros( (len(y), len(set(y))))\n",
    "y_[:, y[:]] = 1\n",
    "y_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
