{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def unit_norm_layer_init(input_shape, output_shape):\n",
    "    return np.random.normal(size=(input_shape, output_shape))\n",
    "\n",
    "\n",
    "def ones_layer_init(input_shape, output_shape):\n",
    "    return np.ones((input_shape, output_shape))\n",
    "\n",
    "\n",
    "def zeros_init_layer(length):\n",
    "    return np.zeros(length)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    # https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "    exp = np.exp(x - np.max(x))\n",
    "    return exp / np.sum(exp, axis=1)[:, None]\n",
    "\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def linear_derivative(X):\n",
    "    return np.ones((X.shape[0]))\n",
    "\n",
    "'''\n",
    "def softmax_grad(x):\n",
    "    s = sigmoid(x)\n",
    "    _, K = s.shape\n",
    "    return s @ (np.eye(K, K) - s).T\n",
    "'''\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(x, 0, x)\n",
    "\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x > 0).astype(int)\n",
    "\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "    return -np.sum(y_true * np.log(y_pred)) / N\n",
    "\n",
    "\n",
    "def cross_entropy_derivative(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "    return -y_true/y_pred / N\n",
    "\n",
    "\n",
    "def one_hot_encode(y, n_classes):\n",
    "    y_onehot = np.zeros((len(y), n_classes))\n",
    "    for i, y_i in enumerate(y):\n",
    "        y_onehot[i, y_i] = 1\n",
    "    return y_onehot\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hidden=(8, 6),\n",
    "                 init_weights='unit_norm',\n",
    "                 init_bias='zeros',\n",
    "                 activation='sigmoid',\n",
    "                 loss='cross_entropy',\n",
    "                 mode='classification',\n",
    "                 random_state=1):\n",
    "        self.hidden = hidden\n",
    "        self.init_weights = init_weights\n",
    "        self.init_bias = init_bias\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.random_state = random_state\n",
    "        self.mode = mode\n",
    "        np.random.seed(self.random_state)\n",
    "        self._set_act_func()\n",
    "        self._set_loss()\n",
    "        \n",
    "        \n",
    "    def _init_neural_network(self):\n",
    "        implemented_weight_inits = {'unit_norm': unit_norm_layer_init,\n",
    "                                    'ones': ones_layer_init\n",
    "                                   }\n",
    "        implemented_bias_inits = {'zeros': zeros_init_layer,\n",
    "                                   }\n",
    "        try:\n",
    "            init_layer_weight = implemented_weight_inits[self.init_weights]\n",
    "            init_layer_bias = implemented_bias_inits[self.init_bias]\n",
    "        except KeyError:\n",
    "            raise Exception('{} or {} not accepted'.format(self.init_weights,\n",
    "                                                           self.init_bias))\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for layer in range(len(self.hidden) + 1):\n",
    "            if layer == 0:\n",
    "                input_shape = self.n_features\n",
    "                output_shape = self.hidden[layer]\n",
    "            elif layer == len(self.hidden):\n",
    "                input_shape = self.hidden[layer - 1]\n",
    "                output_shape = self.n_classes\n",
    "            else:\n",
    "                input_shape = self.hidden[layer - 1]\n",
    "                output_shape = self.hidden[layer]                \n",
    "            w_l = init_layer_weight(input_shape, output_shape)\n",
    "            b_l = init_layer_bias(output_shape)\n",
    "            self.weights.append(w_l)\n",
    "            self.biases.append(b_l)\n",
    "        \n",
    "            \n",
    "    def _set_act_func(self):\n",
    "        implemented_activations = {'sigmoid': sigmoid,\n",
    "                                   'ReLU': ReLU,\n",
    "                                   'linear': linear_derivative}\n",
    "        # set activation function\n",
    "        try:\n",
    "            self.act = implemented_activations[self.activation]\n",
    "        except KeyError:\n",
    "            raise Exception('{} not accepted'.format(self.activation))\n",
    "            \n",
    "        implemented_derivatives = {'sigmoid': sigmoid_derivative,\n",
    "                                   'ReLU': ReLU_derivative,\n",
    "                                   'linear': linear_derivative}\n",
    "        \n",
    "        # set activation derivative (da/dz)\n",
    "        try:\n",
    "            self.act_derivative = implemented_derivatives[self.activation]\n",
    "        except KeyError:\n",
    "            raise Exception('derivative not implemented for {}'.format(self.activation))\n",
    "\n",
    "        # set activation for last layer (softmax for classification and linear for regression)\n",
    "        if self.mode == 'classification':\n",
    "            self.last_act = softmax\n",
    "        elif self.mode == 'regression':\n",
    "            self.last_act = linear\n",
    "\n",
    "    def _set_loss(self):\n",
    "        implemented_losses = {'cross_entropy': cross_entropy,}\n",
    "        try:\n",
    "            self.loss_func = implemented_losses[self.loss]\n",
    "        except KeyError:\n",
    "            raise Exception('{} not accepted'.format(self.loss))\n",
    "    \n",
    "    def train(self, X, y, n_epochs=10, lr=0.001, n_classes=None):\n",
    "        self.lr = lr\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.classes = n_classes\n",
    "        if n_classes is None:\n",
    "            self.classes = set(y)\n",
    "            self.n_classes = len(self.classes)\n",
    "        \n",
    "        y_one_hot = one_hot_encode(y, self.n_classes)\n",
    "        self._init_neural_network()\n",
    "        \n",
    "        print(self.biases[1])\n",
    "        for e in range(n_epochs):\n",
    "            # implement shuffle\n",
    "            # implement batch\n",
    "            self._feed_forward(X)\n",
    "            self.loss_e = self.loss_func(y_one_hot, self.activations[-1])\n",
    "            print('loss', e, self.loss_e)\n",
    "            self._back_prop(X, y_one_hot)\n",
    "        \n",
    "        print(self.biases[1])\n",
    "                \n",
    "\n",
    "    \n",
    "    def _feed_forward(self, X):\n",
    "        self.activations = []\n",
    "        self.Z_list = []\n",
    "        act = self.act\n",
    "        for layer, (w_l, b_l) in enumerate(zip(self.weights, self.biases)):\n",
    "            if layer == 0:\n",
    "                prev = X\n",
    "            else:\n",
    "                prev = self.activations[-1]\n",
    "\n",
    "            if layer == len(self.hidden):\n",
    "                act = self.last_act\n",
    "            Z_l = np.dot(prev, w_l) + b_l\n",
    "            act_l = act(Z_l)    \n",
    "            self.activations.append(act_l)\n",
    "            self.Z_list.append(Z_l)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        self._feed_forward(X)\n",
    "        return self.activations[-1]\n",
    "            \n",
    "    def _dE_dZ(self, y, p):\n",
    "        # dE/dz where E(y) - cross entropy and a(z) is the softmax activation function\n",
    "        return p - y\n",
    "            \n",
    "    def _back_prop(self, X, y):\n",
    "        y_pred = self.activations[-1]\n",
    "        \n",
    "        new_weights, new_biases = [], []\n",
    "        L = len(self.activations)\n",
    "        for layer in range(L-1, -1, -1):\n",
    "            w_l, b_l = self.weights[layer], self.biases[layer]\n",
    "            Z_l = self.Z_list[layer]\n",
    "            \n",
    "            if layer == 0:\n",
    "                act_prev = X\n",
    "            else:\n",
    "                act_prev = self.activations[layer-1]\n",
    "            \n",
    "            if layer == L-1:\n",
    "                self.dE_dz = self._dE_dZ(y, y_pred)\n",
    "            else:\n",
    "                dE_da = self.dE_dz @ self.weights[layer+1].T # dE_da wrt activation of current layer\n",
    "                da_dz = self.act_derivative(Z_l)\n",
    "                self.dE_dz = np.multiply(da_dz, dE_da)\n",
    "            \n",
    "            dE_dW = act_prev.T @ self.dE_dz\n",
    "            dE_db = np.sum(self.dE_dz, axis=0)\n",
    "            #print(layer, act_prev.T.shape, self.dE_dz.shape, dE_dW.shape, w_l.shape)\n",
    "            w_l -= self.lr * dE_dW\n",
    "            b_l -= self.lr * dE_db\n",
    "            \n",
    "            new_weights.append(w_l)\n",
    "            new_biases.append(b_l)\n",
    "            \n",
    "        self.weights = new_weights[::-1]\n",
    "        self.biases = new_biases[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trn.shape = (100, 2), X_test.shape = (50, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load Iris Dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  \n",
    "y = iris.target\n",
    "\n",
    "\n",
    "feature_idxs = [1, 3] # SET FEATURES BY INDEX <------------------\n",
    "\n",
    "feature_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "xlbl, ylbl = feature_names[feature_idxs[0]], feature_names[feature_idxs[1]] \n",
    "# We will also split the dataset into training and testing so we can evaluate the kNN classifier\n",
    "X_trn_, X_test_, y_trn, y_test = train_test_split(X, \n",
    "                                                 y, \n",
    "                                                 test_size=0.333, \n",
    "                                                 random_state=0,\n",
    "                                                 stratify=y)\n",
    "X_trn, X_test = X_trn_[:, feature_idxs], X_test_[:, feature_idxs]\n",
    "\n",
    "print(\"X_trn.shape = {}, X_test.shape = {}\".format(X_trn.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n",
      "loss 0 5.94952518477602\n",
      "loss 1 5.4802021973966335\n",
      "loss 2 5.032410031042765\n",
      "loss 3 4.6106821170967285\n",
      "loss 4 4.2234712466361195\n",
      "loss 5 3.8832408444212647\n",
      "loss 6 3.6024557359770224\n",
      "loss 7 3.384519418752235\n",
      "loss 8 3.2181016183615028\n",
      "loss 9 3.084200333980726\n",
      "loss 10 2.9672382474854384\n",
      "loss 11 2.858478977981857\n",
      "loss 12 2.75394746343708\n",
      "loss 13 2.652015202987791\n",
      "loss 14 2.5520516291240427\n",
      "loss 15 2.4538304479739264\n",
      "loss 16 2.3572941575469173\n",
      "loss 17 2.2624665692246264\n",
      "loss 18 2.169423460665378\n",
      "loss 19 2.07828586360507\n",
      "loss 20 1.9892222937502595\n",
      "loss 21 1.9024544421194447\n",
      "loss 22 1.8182636725847419\n",
      "loss 23 1.7369963399550317\n",
      "loss 24 1.6590657051731008\n",
      "loss 25 1.5849476345801838\n",
      "loss 26 1.5151667442623178\n",
      "loss 27 1.4502697087553589\n",
      "loss 28 1.3907837658615205\n",
      "loss 29 1.337161621844385\n",
      "loss 30 1.2897189972294614\n",
      "loss 31 1.248576665409246\n",
      "loss 32 1.2136222447011271\n",
      "loss 33 1.1845050642026873\n",
      "loss 34 1.160669057662905\n",
      "loss 35 1.1414167562387185\n",
      "loss 36 1.1259878895226294\n",
      "loss 37 1.1136336466699908\n",
      "loss 38 1.103672596314097\n",
      "loss 39 1.0955228036111366\n",
      "loss 40 1.0887122342664386\n",
      "loss 41 1.0828736191229715\n",
      "loss 42 1.0777306315455661\n",
      "loss 43 1.073080883430332\n",
      "loss 44 1.0687792814970658\n",
      "loss 45 1.064723554316175\n",
      "loss 46 1.0608425636640748\n",
      "loss 47 1.0570873291964507\n",
      "loss 48 1.053424381673198\n",
      "loss 49 1.0498309706634057\n",
      "loss 50 1.0462916784185365\n",
      "loss 51 1.0427960640098979\n",
      "loss 52 1.039337043734133\n",
      "loss 53 1.0359097880059032\n",
      "loss 54 1.0325109754969404\n",
      "loss 55 1.0291382917179015\n",
      "loss 56 1.0257900934596345\n",
      "loss 57 1.0224651850366542\n",
      "loss 58 1.019162669505172\n",
      "loss 59 1.0158818499533095\n",
      "loss 60 1.012622164123071\n",
      "loss 61 1.009383141162114\n",
      "loss 62 1.0061643730366903\n",
      "loss 63 1.0029654956407137\n",
      "loss 64 0.9997861763079166\n",
      "loss 65 0.9966261055471844\n",
      "loss 66 0.9934849915602644\n",
      "loss 67 0.9903625565908172\n",
      "loss 68 0.9872585344777884\n",
      "loss 69 0.9841726690001218\n",
      "loss 70 0.9811047127410799\n",
      "loss 71 0.9780544262935493\n",
      "loss 72 0.9750215776890285\n",
      "loss 73 0.9720059419733481\n",
      "loss 74 0.9690073008787138\n",
      "loss 75 0.9660254425590834\n",
      "loss 76 0.9630601613673363\n",
      "loss 77 0.9601112576601918\n",
      "loss 78 0.9571785376217409\n",
      "loss 79 0.9542618130996692\n",
      "loss 80 0.951360901450345\n",
      "loss 81 0.9484756253903075\n",
      "loss 82 0.9456058128525737\n",
      "loss 83 0.9427512968467666\n",
      "loss 84 0.9399119153224196\n",
      "loss 85 0.9370875110350668\n",
      "loss 86 0.9342779314148589\n",
      "loss 87 0.93148302843756\n",
      "loss 88 0.9287026584978224\n",
      "loss 89 0.9259366822846846\n",
      "loss 90 0.9231849646592475\n",
      "loss 91 0.9204473745345119\n",
      "loss 92 0.9177237847573477\n",
      "loss 93 0.9150140719925898\n",
      "loss 94 0.912318116609236\n",
      "loss 95 0.9096358025687433\n",
      "loss 96 0.9069670173153986\n",
      "loss 97 0.904311651668757\n",
      "loss 98 0.9016695997181281\n",
      "loss 99 0.899040758719095\n",
      "[ 0.02601067  0.1209011  -0.14691177]\n",
      "trn acc 0.67\n",
      "test acc 0.66\n"
     ]
    }
   ],
   "source": [
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "nn = NeuralNetwork(hidden=(6,), \n",
    "                   init_weights='unit_norm', \n",
    "                   activation='ReLU',\n",
    "                   )\n",
    "nn.train(X_trn, y_trn, n_epochs=100, lr=0.0001)\n",
    "y_pred_trn = nn.predict(X_trn).argmax(axis=1)\n",
    "y_pred_test = nn.predict(X_test).argmax(axis=1)\n",
    "\n",
    "print('trn acc', accuracy_score(y_pred_trn, y_trn))\n",
    "\n",
    "print('test acc', accuracy_score(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Wi[1] - nn.Wf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Bi[0] - nn.Bf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02601067,  0.1209011 , -0.14691177])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.biases[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100,), (100,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_trn.shape, y_trn.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
