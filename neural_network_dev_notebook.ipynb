{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unit_norm_layer_init(input_shape, output_shape):\n",
    "    return np.random.normal(size=(input_shape, output_shape))\n",
    "\n",
    "\n",
    "def ones_layer_init(input_shape, output_shape):\n",
    "    return np.ones((input_shape, output_shape))\n",
    "\n",
    "\n",
    "def zeros_init_layer(length):\n",
    "    return np.zeros(length)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    # https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "    exp = np.exp(x - np.max(x))\n",
    "    return exp / np.sum(exp, axis=1)[:, None]\n",
    "\n",
    "\n",
    "def softmax_gradient(z, sm=None):\n",
    "    # https://stackoverflow.com/questions/57741998/vectorizing-softmax-cross-entropy-gradient\n",
    "    if sm is None:\n",
    "        sm = softmax(z)\n",
    "    res = np.einsum('ij,ik->ijk', sm, -sm)\n",
    "    np.einsum('ijj->ij',res)[...] += sm\n",
    "    return res\n",
    "\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def linear_derivative(X):\n",
    "    #np.repeat(np.eye(K, K)[np.newaxis, :, :], N, axis=0).shape\n",
    "    return np.ones((X.shape[0]))\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(x, 0, x)\n",
    "\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x > 0).astype(int)\n",
    "\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "    return -np.sum(y_true * np.log(y_pred)) / N\n",
    "\n",
    "\n",
    "def cross_entropy_derivative(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "    return -(y_true / y_pred) # / N\n",
    "\n",
    "\n",
    "def one_hot_encode(y, n_classes):\n",
    "    y_onehot = np.zeros((len(y), n_classes))\n",
    "    for i, y_i in enumerate(y):\n",
    "        y_onehot[i, y_i] = 1\n",
    "    return y_onehot\n",
    "\n",
    "\n",
    "def softmax_gradient(z,sm=None):\n",
    "    if sm is None:\n",
    "        sm = softmax(z)\n",
    "    res = np.einsum('ij,ik->ijk',sm,-sm)\n",
    "    np.einsum('ijj->ij',res)[...] += sm\n",
    "    return res\n",
    "\n",
    "def dE_dz__(y, z, sm=None):\n",
    "    if sm is None:\n",
    "        sm = softmax(z)\n",
    "    dE_da = cross_entropy_derivative(y, sm)\n",
    "    da_dz = softmax_gradient(z, sm)\n",
    "    return np.einsum('ij,ijk->ik', dE_da, da_dz)\n",
    "\n",
    "\n",
    "def normalize_trn_data(X):\n",
    "    \"\"\"\n",
    "    normalize data to have zero mean and unit variance\n",
    "    :param X: input data (array) - X.shape = (n_samples, m_features)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mean, std = X.mean(axis=0), X.std(axis=0)\n",
    "    return (X - mean) / std, (mean, std)\n",
    "\n",
    "\n",
    "def shuffle_data(X, y):\n",
    "    idx = np.arange(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "def get_batch(X, y, batch_size):\n",
    "    N, _ = X.shape\n",
    "    batch_idxs = np.arange(0, N, batch_size)\n",
    "\n",
    "    for start in batch_idxs:\n",
    "        stop = start + batch_size\n",
    "        X_batch, y_batch = X[start:stop], y[start:stop]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hidden=(8, 6),\n",
    "                 init_weights='unit_norm',\n",
    "                 init_bias='zeros',\n",
    "                 activation='ReLU',\n",
    "                 loss='cross_entropy',\n",
    "                 mode='classification',\n",
    "                 shuffle=True,\n",
    "                 verbose=False,\n",
    "                 batch_size=10,\n",
    "                 random_state=1):\n",
    "        self.hidden = hidden\n",
    "        self.init_weights = init_weights\n",
    "        self.init_bias = init_bias\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.random_state = random_state\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(self.random_state)\n",
    "        self._set_act_func()\n",
    "        self._set_loss()\n",
    "        \n",
    "        \n",
    "    def _init_neural_network(self):\n",
    "        implemented_weight_inits = {'unit_norm': unit_norm_layer_init,\n",
    "                                    'ones': ones_layer_init\n",
    "                                   }\n",
    "        implemented_bias_inits = {'zeros': zeros_init_layer,\n",
    "                                   }\n",
    "        try:\n",
    "            init_layer_weight = implemented_weight_inits[self.init_weights]\n",
    "            init_layer_bias = implemented_bias_inits[self.init_bias]\n",
    "        except KeyError:\n",
    "            raise Exception('{} or {} not accepted'.format(self.init_weights,\n",
    "                                                           self.init_bias))\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for layer in range(len(self.hidden) + 1):\n",
    "            if layer == 0:\n",
    "                input_shape = self.n_features\n",
    "                output_shape = self.hidden[layer]\n",
    "            elif layer == len(self.hidden):\n",
    "                input_shape = self.hidden[layer - 1]\n",
    "                output_shape = self.n_classes\n",
    "            else:\n",
    "                input_shape = self.hidden[layer - 1]\n",
    "                output_shape = self.hidden[layer]                \n",
    "            w_l = init_layer_weight(input_shape, output_shape)\n",
    "            b_l = init_layer_bias(output_shape)\n",
    "            self.weights.append(w_l)\n",
    "            self.biases.append(b_l)\n",
    "        \n",
    "            \n",
    "    def _set_act_func(self):\n",
    "        implemented_activations = {'sigmoid': sigmoid,\n",
    "                                   'ReLU': ReLU,\n",
    "                                   'linear': linear_derivative, \n",
    "                                    'softmax': softmax}\n",
    "        # set activation function\n",
    "        try:\n",
    "            self.act = implemented_activations[self.activation]\n",
    "        except KeyError:\n",
    "            raise Exception('{} not accepted'.format(self.activation))\n",
    "            \n",
    "        implemented_derivatives = {'sigmoid': sigmoid_derivative,\n",
    "                                   'ReLU': ReLU_derivative,\n",
    "                                   'linear': linear_derivative,\n",
    "                                    'softmax': softmax_derivative}\n",
    "        \n",
    "        # set activation derivative (da/dz)\n",
    "        try:\n",
    "            self.act_derivative = implemented_derivatives[self.activation]\n",
    "        except KeyError:\n",
    "            raise Exception('derivative not implemented for {}'.format(self.activation))\n",
    "\n",
    "        # set activation for last layer (softmax for classification and linear for regression)\n",
    "        if self.mode == 'classification':\n",
    "            self.last_act = softmax\n",
    "            self.last_act_grad = softmax_gradient\n",
    "        elif self.mode == 'regression':\n",
    "            self.last_act = linear\n",
    "            self.last_act_grad = linear_gradient\n",
    "\n",
    "\n",
    "    def _set_loss(self):\n",
    "        implemented_losses = {'cross_entropy': cross_entropy,}\n",
    "        loss_gradients = {'cross_entropy': cross_entropy_derivative,}\n",
    "        try:\n",
    "            self.loss_func = implemented_losses[self.loss]\n",
    "            self.loss_grad_func = loss_gradients[self.loss]\n",
    "        except KeyError:\n",
    "            raise Exception('{} not accepted'.format(self.loss))\n",
    "    \n",
    "    def train(self, X, y, n_epochs=10, lr=0.001, n_classes=None):\n",
    "        self.lr = lr\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.classes = n_classes\n",
    "        if n_classes is None:\n",
    "            self.classes = set(y)\n",
    "            self.n_classes = len(self.classes)\n",
    "        \n",
    "        y_one_hot = one_hot_encode(y, self.n_classes)\n",
    "        self._init_neural_network()\n",
    "        \n",
    "        \n",
    "        for e in range(n_epochs):\n",
    "            self.loss_e = 0\n",
    "            # shuffle data\n",
    "            if self.shuffle:\n",
    "                X, y_one_hot = shuffle_data(X, y_one_hot)\n",
    "            # iterate through batches\n",
    "            for X_batch, y_batch in get_batch(X, y_one_hot, self.batch_size):\n",
    "                self._feed_forward(X_batch)\n",
    "                self._back_prop(X_batch, y_batch)\n",
    "                self.loss_batch = self.loss_func(y_batch, self.activations[-1])\n",
    "                self.loss_e += self.loss_batch\n",
    "                \n",
    "        if self.verbose:\n",
    "            print(e, 'trn loss = {}'.format(self.loss_e))\n",
    "        print('epoch {}: final trn loss = {}'.format(e, self.loss_e))\n",
    "            \n",
    "    \n",
    "    def _feed_forward(self, X):\n",
    "        self.activations = []\n",
    "        self.Z_list = []\n",
    "        act = self.act\n",
    "        for layer, (w_l, b_l) in enumerate(zip(self.weights, self.biases)):\n",
    "            if layer == 0:\n",
    "                prev = X\n",
    "            else:\n",
    "                prev = self.activations[-1]\n",
    "\n",
    "            if layer == len(self.hidden):\n",
    "                act = self.last_act\n",
    "            Z_l = np.dot(prev, w_l) + b_l\n",
    "            act_l = act(Z_l)    \n",
    "            self.activations.append(act_l)\n",
    "            self.Z_list.append(Z_l)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        self._feed_forward(X)\n",
    "        return self.activations[-1]\n",
    "    \n",
    "    \n",
    "    def _get_gradient(self, y, a, z):\n",
    "        # https://stackoverflow.com/questions/57741998/vectorizing-softmax-cross-entropy-gradient\n",
    "        dE_da = self.loss_grad_func(y, a)\n",
    "        da_dz = self.last_act_grad(z)\n",
    "        return np.einsum('ij,ijk->ik', dE_da, da_dz)\n",
    "    '''\n",
    "    def _dE_dZ(self, y, p):\n",
    "        # dE/dz where E(y) - cross entropy and a(z) is the softmax activation function\n",
    "        return p - y\n",
    "    \n",
    "    def _get_grad(dE_da, da_dz):\n",
    "        return np.tensordot(dE_da, da_dz, axes=([-1],[0]))\n",
    "    '''\n",
    "    def _back_prop(self, X, y):\n",
    "        y_pred = self.activations[-1]\n",
    "        z_last = self.Z_list[-1]\n",
    "        self.dE_dz = self._get_gradient(y, y_pred, z_last)\n",
    "        #self.dE_da = self.loss_grad_func(y, y_pred)\n",
    "        #self.da_dz = self.last_act_grad(z_last) # gradient of last activation layer\n",
    "        #self.dE_dz = get_dE_dz(self.dE_da, self.da_dz) # gradient at the last layer\n",
    "        #self.dE_dz_1 = self.dE_dz[...]\n",
    "        #self.dE_dz_2 = self._dE_dZ(y, y_pred)[...]\n",
    "\n",
    "        new_weights, new_biases = [], []\n",
    "        L = len(self.activations)\n",
    "        for layer in range(L-1, -1, -1):\n",
    "            w_l, b_l = self.weights[layer], self.biases[layer]\n",
    "            Z_l = self.Z_list[layer]\n",
    "            \n",
    "            if layer == 0:\n",
    "                act_prev = X\n",
    "            else:\n",
    "                act_prev = self.activations[layer-1]\n",
    "            \n",
    "            if layer < L-1:\n",
    "                dE_da = self.dE_dz @ self.weights[layer+1].T # dE_da wrt activation of current layer\n",
    "                da_dz = self.act_derivative(Z_l)\n",
    "                self.dE_dz = np.multiply(da_dz, dE_da)\n",
    "            \n",
    "            dE_dW = act_prev.T @ self.dE_dz\n",
    "            dE_db = np.sum(self.dE_dz, axis=0)\n",
    "            w_l -= self.lr * dE_dW\n",
    "            b_l -= self.lr * dE_db\n",
    "            \n",
    "            new_weights.append(w_l)\n",
    "            new_biases.append(b_l)\n",
    "            \n",
    "        self.weights = new_weights[::-1]\n",
    "        self.biases = new_biases[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trn.shape = (100, 2), X_test.shape = (50, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load Iris Dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  \n",
    "y = iris.target\n",
    "\n",
    "\n",
    "feature_idxs = [1, 3] # SET FEATURES BY INDEX <------------------\n",
    "\n",
    "feature_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "xlbl, ylbl = feature_names[feature_idxs[0]], feature_names[feature_idxs[1]] \n",
    "# We will also split the dataset into training and testing so we can evaluate the kNN classifier\n",
    "X_trn_, X_test_, y_trn, y_test = train_test_split(X, \n",
    "                                                 y, \n",
    "                                                 test_size=0.333, \n",
    "                                                 random_state=0,\n",
    "                                                 stratify=y)\n",
    "X_trn, X_test = X_trn_[:, feature_idxs], X_test_[:, feature_idxs]\n",
    "\n",
    "print(\"X_trn.shape = {}, X_test.shape = {}\".format(X_trn.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trn.shape = (100, 2), X_test.shape = (50, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load Iris Dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  \n",
    "y = iris.target\n",
    "\n",
    "\n",
    "feature_idxs = [1, 3] # SET FEATURES BY INDEX <------------------\n",
    "\n",
    "feature_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "xlbl, ylbl = feature_names[feature_idxs[0]], feature_names[feature_idxs[1]] \n",
    "# We will also split the dataset into training and testing so we can evaluate the kNN classifier\n",
    "X_trn_, X_test_, y_trn, y_test = train_test_split(X, \n",
    "                                                 y, \n",
    "                                                 test_size=0.333, \n",
    "                                                 random_state=0,\n",
    "                                                 stratify=y)\n",
    "X_trn, X_test = X_trn_[:, feature_idxs], X_test_[:, feature_idxs]\n",
    "\n",
    "print(\"X_trn.shape = {}, X_test.shape = {}\".format(X_trn.shape, X_test.shape))\n",
    "X_trn_norm, (trn_mean, trn_std) = normalize_trn_data(X_trn)\n",
    "X_test_norm = (X_test - trn_mean) / trn_std\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99: final trn loss = 0.1913208082881778\n",
      "trn acc 0.97\n",
      "test acc 0.94\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(hidden=(6,), \n",
    "                   init_weights='unit_norm', \n",
    "                   activation='ReLU',\n",
    "                   shuffle=True,\n",
    "                   batch_size=50,\n",
    "                   random_state=1\n",
    "                   )\n",
    "nn.train(X_trn_norm, y_trn, n_epochs=100, lr=0.01)\n",
    "y_pred_trn = nn.predict(X_trn_norm).argmax(axis=1)\n",
    "y_pred_test = nn.predict(X_test_norm).argmax(axis=1)\n",
    "\n",
    "print('trn acc', accuracy_score(y_pred_trn, y_trn))\n",
    "print('test acc', accuracy_score(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn acc 0.9\n",
      "test acc 0.82\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(6, activation='relu', input_dim=X_trn_norm.shape[1]))\n",
    "model.add(Dense(3, activation='softmax',))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=0.0, momentum=0.00, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_trn_norm, one_hot_encode(y_trn, 3),\n",
    "          epochs=100,\n",
    "          batch_size=50,\n",
    "          verbose=False)\n",
    "y_pred_trn = model.predict(X_trn_norm).argmax(axis=1)\n",
    "y_pred_test = model.predict(X_test_norm).argmax(axis=1)\n",
    "\n",
    "\n",
    "print('trn acc', accuracy_score(y_pred_trn, y_trn))\n",
    "print('test acc', accuracy_score(y_pred_test, y_test))\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
