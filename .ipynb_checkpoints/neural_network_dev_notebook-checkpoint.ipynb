{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_layer_weight_unit_norm(input_shape, output_shape):\n",
    "    return np.random.normal(size=(input_shape, output_shape))\n",
    "\n",
    "\n",
    "def init_layer_weights_ones(input_shape, output_shape):\n",
    "    return np.ones((input_shape, output_shape))\n",
    "\n",
    "\n",
    "def init_layer_bias(length):\n",
    "    return np.zeros(length)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    # https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "    exp = np.exp(x - np.max(x))\n",
    "    return exp / np.sum(exp, axis=1)[:, None]\n",
    "\n",
    "\n",
    "def softmax_gradient(z, sm=None):\n",
    "    # https://stackoverflow.com/questions/57741998/vectorizing-softmax-cross-entropy-gradient\n",
    "    if sm is None:\n",
    "        sm = softmax(z)\n",
    "    res = np.einsum('ij,ik->ijk', sm, -sm)\n",
    "    np.einsum('ijj->ij',res)[...] += sm\n",
    "    return res\n",
    "\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def linear_derivative(X):\n",
    "    #np.repeat(np.eye(K, K)[np.newaxis, :, :], N, axis=0).shape\n",
    "    return np.ones((X.shape[0]))\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(x, 0, x)\n",
    "\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x > 0).astype(int)\n",
    "\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "    return -np.sum(y_true * np.log(y_pred)) / N\n",
    "\n",
    "\n",
    "def cross_entropy_derivative(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "    return -(y_true / y_pred) # / N\n",
    "\n",
    "\n",
    "def one_hot_encode(y, n_classes):\n",
    "    y_onehot = np.zeros((len(y), n_classes))\n",
    "    for i, y_i in enumerate(y):\n",
    "        y_onehot[i, y_i] = 1\n",
    "    return y_onehot\n",
    "\n",
    "\n",
    "def softmax_gradient(z,sm=None):\n",
    "    if sm is None:\n",
    "        sm = softmax(z)\n",
    "    res = np.einsum('ij,ik->ijk',sm,-sm)\n",
    "    np.einsum('ijj->ij',res)[...] += sm\n",
    "    return res\n",
    "\n",
    "def dE_dz__(y, z, sm=None):\n",
    "    if sm is None:\n",
    "        sm = softmax(z)\n",
    "    dE_da = cross_entropy_derivative(y, sm)\n",
    "    da_dz = softmax_gradient(z, sm)\n",
    "    return np.einsum('ij,ijk->ik', dE_da, da_dz)\n",
    "\n",
    "\n",
    "def norm_data(X):\n",
    "    \"\"\"\n",
    "    normalize data to have zero mean and unit variance\n",
    "    :param X: input data (array) - X.shape = (n_samples, m_features)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mean, std = X.mean(axis=0), X.std(axis=0)\n",
    "    return (X - mean) / std, (mean, std)\n",
    "\n",
    "\n",
    "def shuffle_data(X, y):\n",
    "    idx = np.arange(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "def batch_iterator(X, y, batch_size):\n",
    "    N, _ = X.shape\n",
    "    batch_idxs = np.arange(0, N, batch_size)\n",
    "\n",
    "    for start in batch_idxs:\n",
    "        stop = start + batch_size\n",
    "        X_batch, y_batch = X[start:stop], y[start:stop]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hidden=(8, 6),\n",
    "                 init_weights='unit_norm',\n",
    "                 init_bias='zeros',\n",
    "                 activation='ReLU',\n",
    "                 loss='cross_entropy',\n",
    "                 mode='classification',\n",
    "                 shuffle=True,\n",
    "                 verbose=False,\n",
    "                 batch_size=10,\n",
    "                 random_state=1):\n",
    "        self.hidden = hidden\n",
    "        self.init_weights = init_weights\n",
    "        self.init_bias = init_bias\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.random_state = random_state\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(self.random_state)\n",
    "        self._set_act_func()\n",
    "        self._set_loss()\n",
    "        \n",
    "        \n",
    "    def _init_neural_network(self):\n",
    "        implemented_weight_inits = {'unit_norm': init_layer_weight_unit_norm,\n",
    "                                    'ones': init_layer_weights_ones\n",
    "                                   }\n",
    "        implemented_bias_inits = {'zeros': init_layer_bias,\n",
    "                                   }\n",
    "        try:\n",
    "            init_layer_weight = implemented_weight_inits[self.init_weights]\n",
    "            init_layer_bias = implemented_bias_inits[self.init_bias]\n",
    "        except KeyError:\n",
    "            raise Exception('{} or {} not accepted'.format(self.init_weights,\n",
    "                                                           self.init_bias))\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for layer in range(len(self.hidden) + 1):\n",
    "            if layer == 0:\n",
    "                input_shape = self.n_features\n",
    "                output_shape = self.hidden[layer]\n",
    "            elif layer == len(self.hidden):\n",
    "                input_shape = self.hidden[layer - 1]\n",
    "                output_shape = self.n_classes\n",
    "            else:\n",
    "                input_shape = self.hidden[layer - 1]\n",
    "                output_shape = self.hidden[layer]                \n",
    "            w_l = init_layer_weight(input_shape, output_shape)\n",
    "            b_l = init_layer_bias(output_shape)\n",
    "            self.weights.append(w_l)\n",
    "            self.biases.append(b_l)\n",
    "        \n",
    "            \n",
    "    def _set_act_func(self):\n",
    "        implemented_activations = {'sigmoid': sigmoid,\n",
    "                                   'ReLU': ReLU,\n",
    "                                   'linear': linear_derivative, \n",
    "                                    'softmax': softmax}\n",
    "        # set activation function\n",
    "        try:\n",
    "            self.act = implemented_activations[self.activation]\n",
    "        except KeyError:\n",
    "            raise Exception('{} not accepted'.format(self.activation))\n",
    "            \n",
    "        implemented_derivatives = {'sigmoid': sigmoid_derivative,\n",
    "                                   'ReLU': ReLU_derivative,\n",
    "                                   'linear': linear_derivative,\n",
    "                                    'softmax': softmax_derivative}\n",
    "        \n",
    "        # set activation derivative (da/dz)\n",
    "        try:\n",
    "            self.act_derivative = implemented_derivatives[self.activation]\n",
    "        except KeyError:\n",
    "            raise Exception('derivative not implemented for {}'.format(self.activation))\n",
    "\n",
    "        # set activation for last layer (softmax for classification and linear for regression)\n",
    "        if self.mode == 'classification':\n",
    "            self.last_act = softmax\n",
    "            self.last_act_grad = softmax_gradient\n",
    "        elif self.mode == 'regression':\n",
    "            self.last_act = linear\n",
    "            self.last_act_grad = linear_gradient\n",
    "\n",
    "\n",
    "    def _set_loss(self):\n",
    "        implemented_losses = {'cross_entropy': cross_entropy,}\n",
    "        loss_gradients = {'cross_entropy': cross_entropy_derivative,}\n",
    "        try:\n",
    "            self.loss_func = implemented_losses[self.loss]\n",
    "            self.loss_grad_func = loss_gradients[self.loss]\n",
    "        except KeyError:\n",
    "            raise Exception('{} not accepted'.format(self.loss))\n",
    "    \n",
    "    def train(self, X, y, n_epochs=10, lr=0.001, n_classes=None):\n",
    "        self.lr = lr\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.classes = n_classes\n",
    "        if n_classes is None:\n",
    "            self.classes = set(y)\n",
    "            self.n_classes = len(self.classes)\n",
    "        \n",
    "        y_one_hot = one_hot_encode(y, self.n_classes)\n",
    "        self._init_neural_network()\n",
    "        \n",
    "        \n",
    "        for e in range(n_epochs):\n",
    "            self.loss_e = 0\n",
    "            # shuffle data\n",
    "            if self.shuffle:\n",
    "                X, y_one_hot = shuffle_data(X, y_one_hot)\n",
    "            # iterate through batches\n",
    "            for X_batch, y_batch in batch_iterator(X, y_one_hot, self.batch_size):\n",
    "                self._feed_forward(X_batch)\n",
    "                self._back_prop(X_batch, y_batch)\n",
    "                self.loss_batch = self.loss_func(y_batch, self.activations[-1])\n",
    "                self.loss_e += self.loss_batch\n",
    "                \n",
    "        if self.verbose:\n",
    "            print(e, 'trn loss = {}'.format(self.loss_e))\n",
    "        print('epoch {}: final trn loss = {}'.format(e, self.loss_e))\n",
    "            \n",
    "    \n",
    "    def _feed_forward(self, X):\n",
    "        self.activations = []\n",
    "        self.Z_list = []\n",
    "        act = self.act\n",
    "        for layer, (w_l, b_l) in enumerate(zip(self.weights, self.biases)):\n",
    "            if layer == 0:\n",
    "                prev = X\n",
    "            else:\n",
    "                prev = self.activations[-1]\n",
    "\n",
    "            if layer == len(self.hidden):\n",
    "                act = self.last_act\n",
    "            Z_l = np.dot(prev, w_l) + b_l\n",
    "            act_l = act(Z_l)    \n",
    "            self.activations.append(act_l)\n",
    "            self.Z_list.append(Z_l)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        self._feed_forward(X)\n",
    "        return self.activations[-1]\n",
    "    \n",
    "    \n",
    "    def _get_gradient(self, y, a, z):\n",
    "        # https://stackoverflow.com/questions/57741998/vectorizing-softmax-cross-entropy-gradient\n",
    "        dE_da = self.loss_grad_func(y, a)\n",
    "        da_dz = self.last_act_grad(z)\n",
    "        return np.einsum('ij,ijk->ik', dE_da, da_dz)\n",
    "    '''\n",
    "    def _dE_dZ(self, y, p):\n",
    "        # dE/dz where E(y) - cross entropy and a(z) is the softmax activation function\n",
    "        return p - y\n",
    "    \n",
    "    def _get_grad(dE_da, da_dz):\n",
    "        return np.tensordot(dE_da, da_dz, axes=([-1],[0]))\n",
    "    '''\n",
    "    def _back_prop(self, X, y):\n",
    "        y_pred = self.activations[-1]\n",
    "        z_last = self.Z_list[-1]\n",
    "        self.dE_dz = self._get_gradient(y, y_pred, z_last)\n",
    "        #self.dE_da = self.loss_grad_func(y, y_pred)\n",
    "        #self.da_dz = self.last_act_grad(z_last) # gradient of last activation layer\n",
    "        #self.dE_dz = get_dE_dz(self.dE_da, self.da_dz) # gradient at the last layer\n",
    "        #self.dE_dz_1 = self.dE_dz[...]\n",
    "        #self.dE_dz_2 = self._dE_dZ(y, y_pred)[...]\n",
    "\n",
    "        new_weights, new_biases = [], []\n",
    "        L = len(self.activations)\n",
    "        for layer in range(L-1, -1, -1):\n",
    "            w_l, b_l = self.weights[layer], self.biases[layer]\n",
    "            Z_l = self.Z_list[layer]\n",
    "            \n",
    "            if layer == 0:\n",
    "                act_prev = X\n",
    "            else:\n",
    "                act_prev = self.activations[layer-1]\n",
    "            \n",
    "            if layer < L-1:\n",
    "                dE_da = self.dE_dz @ self.weights[layer+1].T # dE_da wrt activation of current layer\n",
    "                da_dz = self.act_derivative(Z_l)\n",
    "                self.dE_dz = np.multiply(da_dz, dE_da)\n",
    "            \n",
    "            dE_dW = act_prev.T @ self.dE_dz\n",
    "            dE_db = np.sum(self.dE_dz, axis=0)\n",
    "            w_l -= self.lr * dE_dW\n",
    "            b_l -= self.lr * dE_db\n",
    "            \n",
    "            new_weights.append(w_l)\n",
    "            new_biases.append(b_l)\n",
    "            \n",
    "        self.weights = new_weights[::-1]\n",
    "        self.biases = new_biases[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trn.shape = (100, 2), X_test.shape = (50, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load Iris Dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  \n",
    "y = iris.target\n",
    "\n",
    "\n",
    "feature_idxs = [1, 3] # SET FEATURES BY INDEX <------------------\n",
    "\n",
    "feature_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "xlbl, ylbl = feature_names[feature_idxs[0]], feature_names[feature_idxs[1]] \n",
    "# We will also split the dataset into training and testing so we can evaluate the kNN classifier\n",
    "X_trn_, X_test_, y_trn, y_test = train_test_split(X, \n",
    "                                                 y, \n",
    "                                                 test_size=0.333, \n",
    "                                                 random_state=0,\n",
    "                                                 stratify=y)\n",
    "X_trn, X_test = X_trn_[:, feature_idxs], X_test_[:, feature_idxs]\n",
    "\n",
    "print(\"X_trn.shape = {}, X_test.shape = {}\".format(X_trn.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trn.shape = (100, 2), X_test.shape = (50, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load Iris Dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  \n",
    "y = iris.target\n",
    "\n",
    "\n",
    "feature_idxs = [1, 3] # SET FEATURES BY INDEX <------------------\n",
    "\n",
    "feature_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "xlbl, ylbl = feature_names[feature_idxs[0]], feature_names[feature_idxs[1]] \n",
    "# We will also split the dataset into training and testing so we can evaluate the kNN classifier\n",
    "X_trn_, X_test_, y_trn, y_test = train_test_split(X, \n",
    "                                                 y, \n",
    "                                                 test_size=0.333, \n",
    "                                                 random_state=0,\n",
    "                                                 stratify=y)\n",
    "X_trn, X_test = X_trn_[:, feature_idxs], X_test_[:, feature_idxs]\n",
    "\n",
    "print(\"X_trn.shape = {}, X_test.shape = {}\".format(X_trn.shape, X_test.shape))\n",
    "X_trn_norm, (trn_mean, trn_std) = norm_data(X_trn)\n",
    "X_test_norm = (X_test - trn_mean) / trn_std\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99: final trn loss = 0.1913208082881778\n",
      "trn acc 0.97\n",
      "test acc 0.94\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(hidden=(6,), \n",
    "                   init_weights='unit_norm', \n",
    "                   activation='ReLU',\n",
    "                   shuffle=True,\n",
    "                   batch_size=50,\n",
    "                   random_state=1\n",
    "                   )\n",
    "nn.train(X_trn_norm, y_trn, n_epochs=100, lr=0.01)\n",
    "y_pred_trn = nn.predict(X_trn_norm).argmax(axis=1)\n",
    "y_pred_test = nn.predict(X_test_norm).argmax(axis=1)\n",
    "\n",
    "print('trn acc', accuracy_score(y_pred_trn, y_trn))\n",
    "print('test acc', accuracy_score(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 1.0853 - acc: 0.2900\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 1.0606 - acc: 0.3600\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 45us/step - loss: 1.0405 - acc: 0.3600\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 1.0210 - acc: 0.5700\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 72us/step - loss: 1.0046 - acc: 0.5600\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.9892 - acc: 0.5600\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 45us/step - loss: 0.9754 - acc: 0.5800\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 45us/step - loss: 0.9609 - acc: 0.5900\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 54us/step - loss: 0.9468 - acc: 0.6000\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 55us/step - loss: 0.9324 - acc: 0.6300\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.9198 - acc: 0.6300\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 55us/step - loss: 0.9061 - acc: 0.6400\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.8931 - acc: 0.6400\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 39us/step - loss: 0.8804 - acc: 0.6600\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 43us/step - loss: 0.8678 - acc: 0.6600\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.8547 - acc: 0.6600\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.8413 - acc: 0.6600\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 45us/step - loss: 0.8294 - acc: 0.6700\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.8147 - acc: 0.6800\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 55us/step - loss: 0.8009 - acc: 0.6900\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 41us/step - loss: 0.7880 - acc: 0.7100\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.7750 - acc: 0.7100\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.7592 - acc: 0.7200\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 59us/step - loss: 0.7444 - acc: 0.7200\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.7293 - acc: 0.7200\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 49us/step - loss: 0.7098 - acc: 0.7600\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.6925 - acc: 0.7600\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.6738 - acc: 0.7600\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.6556 - acc: 0.7700\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 47us/step - loss: 0.6366 - acc: 0.7700\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.6179 - acc: 0.8300\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.6001 - acc: 0.8300\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 55us/step - loss: 0.5815 - acc: 0.8300\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 44us/step - loss: 0.5632 - acc: 0.8700\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.5453 - acc: 0.8700\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.5277 - acc: 0.8800\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 54us/step - loss: 0.5110 - acc: 0.9000\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 55us/step - loss: 0.4953 - acc: 0.9000\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 45us/step - loss: 0.4783 - acc: 0.9000\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 35us/step - loss: 0.4633 - acc: 0.9000\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.4477 - acc: 0.9100\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 67us/step - loss: 0.4341 - acc: 0.9100\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 45us/step - loss: 0.4223 - acc: 0.9100\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 45us/step - loss: 0.4094 - acc: 0.9100\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 55us/step - loss: 0.3975 - acc: 0.9100\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.3869 - acc: 0.9200\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.3771 - acc: 0.9200\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.3680 - acc: 0.9200\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.3581 - acc: 0.9200\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.3501 - acc: 0.9200\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.3417 - acc: 0.9200\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.3344 - acc: 0.9200\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.3273 - acc: 0.9200\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 35us/step - loss: 0.3203 - acc: 0.9200\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.3142 - acc: 0.9200\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3081 - acc: 0.9200\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.3027 - acc: 0.9200\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2975 - acc: 0.9200\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2922 - acc: 0.9400\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.2869 - acc: 0.9400\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.2823 - acc: 0.9400\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.2774 - acc: 0.9400\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 45us/step - loss: 0.2733 - acc: 0.9400\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.2686 - acc: 0.9400\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.2646 - acc: 0.9400\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2607 - acc: 0.9400\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 45us/step - loss: 0.2563 - acc: 0.9400\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 41us/step - loss: 0.2528 - acc: 0.9400\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.2484 - acc: 0.9400\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 55us/step - loss: 0.2449 - acc: 0.9400\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 39us/step - loss: 0.2413 - acc: 0.9400\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.2385 - acc: 0.9400\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.2357 - acc: 0.9400\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 49us/step - loss: 0.2325 - acc: 0.9400\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 55us/step - loss: 0.2288 - acc: 0.9400\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 45us/step - loss: 0.2257 - acc: 0.9400\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 42us/step - loss: 0.2227 - acc: 0.9400\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 47us/step - loss: 0.2225 - acc: 0.9400\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 59us/step - loss: 0.2175 - acc: 0.9500\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 53us/step - loss: 0.2151 - acc: 0.9500\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 45us/step - loss: 0.2140 - acc: 0.9500\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.2094 - acc: 0.9500\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.2077 - acc: 0.9500\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 60us/step - loss: 0.2049 - acc: 0.9500\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 45us/step - loss: 0.2030 - acc: 0.9500\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 65us/step - loss: 0.2003 - acc: 0.9500\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 45us/step - loss: 0.1978 - acc: 0.9500\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.1959 - acc: 0.9500\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.1938 - acc: 0.9500\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.1915 - acc: 0.9500\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 0s 55us/step - loss: 0.1897 - acc: 0.9500\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.1875 - acc: 0.9500\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.1858 - acc: 0.9500\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 0s 58us/step - loss: 0.1857 - acc: 0.9600\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.1826 - acc: 0.9500\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 0s 55us/step - loss: 0.1802 - acc: 0.9500\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.1784 - acc: 0.9500\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 0s 45us/step - loss: 0.1773 - acc: 0.9600\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.1755 - acc: 0.9500\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.1736 - acc: 0.9500\n",
      "trn acc 0.95\n",
      "test acc 0.92\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(6, activation='relu', input_dim=X_trn_norm.shape[1]))\n",
    "model.add(Dense(3, activation='softmax',))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=0.0, momentum=0.00, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_trn_norm, one_hot_encode(y_trn, 3),\n",
    "          epochs=100,\n",
    "          batch_size=50)\n",
    "y_pred_trn = model.predict(X_trn_norm).argmax(axis=1)\n",
    "y_pred_test = model.predict(X_test_norm).argmax(axis=1)\n",
    "\n",
    "\n",
    "print('trn acc', accuracy_score(y_pred_trn, y_trn))\n",
    "print('test acc', accuracy_score(y_pred_test, y_test))\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
