{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def unit_norm_layer_init(input_shape, output_shape):\n",
    "    return np.random.normal(size=(input_shape, output_shape))\n",
    "\n",
    "\n",
    "def ones_layer_init(input_shape, output_shape):\n",
    "    return np.ones((input_shape, output_shape))\n",
    "\n",
    "\n",
    "def zeros_init_layer(length):\n",
    "    return np.zeros(length)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    # https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "    exp = np.exp(x - np.max(x))\n",
    "    return exp / np.sum(exp, axis=1)[:, None]\n",
    "\n",
    "def softmax_derivative(Z):\n",
    "    N, K = Z.shape\n",
    "    s = softmax(Z)[:, :, np.newaxis]\n",
    "    a = np.tensordot(s, np.ones((1, K)), axes=([-1],[0]))\n",
    "    I = np.repeat(np.eye(K, K)[np.newaxis, :, :], N, axis=0)\n",
    "    b = I - np.tensordot(np.ones((K, 1)), s.T, axes=([-1],[0])).T\n",
    "    return a * np.swapaxes(b, 1, 2)\n",
    "\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def linear_derivative(X):\n",
    "    #np.repeat(np.eye(K, K)[np.newaxis, :, :], N, axis=0).shape\n",
    "    return np.ones((X.shape[0]))\n",
    "\n",
    "'''\n",
    "def softmax_grad(x):\n",
    "    s = sigmoid(x)\n",
    "    _, K = s.shape\n",
    "    return s @ (np.eye(K, K) - s).T\n",
    "'''\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(x, 0, x)\n",
    "\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x > 0).astype(int)\n",
    "\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "    return -np.sum(y_true * np.log(y_pred)) / N\n",
    "\n",
    "\n",
    "def cross_entropy_derivative(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "    return -(y_true / y_pred) # / N\n",
    "\n",
    "\n",
    "def one_hot_encode(y, n_classes):\n",
    "    y_onehot = np.zeros((len(y), n_classes))\n",
    "    for i, y_i in enumerate(y):\n",
    "        y_onehot[i, y_i] = 1\n",
    "    return y_onehot\n",
    "\n",
    "\n",
    "\n",
    "def get_dE_dz(dE_da, da_dz):\n",
    "    # array (N x K)\n",
    "    # array (N x K x K)\n",
    "    # output: array (N x K)\n",
    "    N, K = dE_da.shape\n",
    "    dE_dz = np.zeros((N, K))\n",
    "    for n in range(N):\n",
    "        dE_dz[n, :] = np.matmul(da_dz[n], dE_da[n, :, np.newaxis]).T\n",
    "    return dE_dz\n",
    "\n",
    "\n",
    "def normalize_trn_data(X):\n",
    "    \"\"\"\n",
    "    normalize data to have zero mean and unit variance\n",
    "    :param X: input data (array) - X.shape = (n_samples, m_features)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mean, std = X.mean(axis=0), X.std(axis=0)\n",
    "    return (X - mean) / std, (mean, std)\n",
    "\n",
    "\n",
    "def shuffle_data(X, y):\n",
    "    idx = np.arange(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    return X[idx, :], y[idx]\n",
    "\n",
    "def get_batch(X, y, batch_size):\n",
    "    N, _ = X.shape\n",
    "    batch_idxs = np.arange(0, N, batch_size)\n",
    "\n",
    "    for start in batch_idxs:\n",
    "        stop = start + batch_size\n",
    "        X_batch, y_batch = X[start:stop], y[start:stop]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hidden=(8, 6),\n",
    "                 init_weights='unit_norm',\n",
    "                 init_bias='zeros',\n",
    "                 activation='ReLU',\n",
    "                 loss='cross_entropy',\n",
    "                 mode='classification',\n",
    "                 shuffle=True,\n",
    "                 verbose=False,\n",
    "                 batch_size=10,\n",
    "                 random_state=1):\n",
    "        self.hidden = hidden\n",
    "        self.init_weights = init_weights\n",
    "        self.init_bias = init_bias\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.random_state = random_state\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(self.random_state)\n",
    "        self._set_act_func()\n",
    "        self._set_loss()\n",
    "        \n",
    "        \n",
    "    def _init_neural_network(self):\n",
    "        implemented_weight_inits = {'unit_norm': unit_norm_layer_init,\n",
    "                                    'ones': ones_layer_init\n",
    "                                   }\n",
    "        implemented_bias_inits = {'zeros': zeros_init_layer,\n",
    "                                   }\n",
    "        try:\n",
    "            init_layer_weight = implemented_weight_inits[self.init_weights]\n",
    "            init_layer_bias = implemented_bias_inits[self.init_bias]\n",
    "        except KeyError:\n",
    "            raise Exception('{} or {} not accepted'.format(self.init_weights,\n",
    "                                                           self.init_bias))\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for layer in range(len(self.hidden) + 1):\n",
    "            if layer == 0:\n",
    "                input_shape = self.n_features\n",
    "                output_shape = self.hidden[layer]\n",
    "            elif layer == len(self.hidden):\n",
    "                input_shape = self.hidden[layer - 1]\n",
    "                output_shape = self.n_classes\n",
    "            else:\n",
    "                input_shape = self.hidden[layer - 1]\n",
    "                output_shape = self.hidden[layer]                \n",
    "            w_l = init_layer_weight(input_shape, output_shape)\n",
    "            b_l = init_layer_bias(output_shape)\n",
    "            self.weights.append(w_l)\n",
    "            self.biases.append(b_l)\n",
    "        \n",
    "            \n",
    "    def _set_act_func(self):\n",
    "        implemented_activations = {'sigmoid': sigmoid,\n",
    "                                   'ReLU': ReLU,\n",
    "                                   'linear': linear_derivative, \n",
    "                                    'softmax': softmax}\n",
    "        # set activation function\n",
    "        try:\n",
    "            self.act = implemented_activations[self.activation]\n",
    "        except KeyError:\n",
    "            raise Exception('{} not accepted'.format(self.activation))\n",
    "            \n",
    "        implemented_derivatives = {'sigmoid': sigmoid_derivative,\n",
    "                                   'ReLU': ReLU_derivative,\n",
    "                                   'linear': linear_derivative,\n",
    "                                    'softmax': softmax_derivative}\n",
    "        \n",
    "        # set activation derivative (da/dz)\n",
    "        try:\n",
    "            self.act_derivative = implemented_derivatives[self.activation]\n",
    "        except KeyError:\n",
    "            raise Exception('derivative not implemented for {}'.format(self.activation))\n",
    "\n",
    "        # set activation for last layer (softmax for classification and linear for regression)\n",
    "        if self.mode == 'classification':\n",
    "            self.last_act = softmax\n",
    "            self.last_act_grad = softmax_derivative\n",
    "        elif self.mode == 'regression':\n",
    "            self.last_act = linear\n",
    "            self.last_act_grad = linear_derivative\n",
    "\n",
    "\n",
    "    def _set_loss(self):\n",
    "        implemented_losses = {'cross_entropy': cross_entropy,}\n",
    "        loss_gradients = {'cross_entropy': cross_entropy_derivative,}\n",
    "        try:\n",
    "            self.loss_func = implemented_losses[self.loss]\n",
    "            self.loss_grad_func = loss_gradients[self.loss]\n",
    "        except KeyError:\n",
    "            raise Exception('{} not accepted'.format(self.loss))\n",
    "    \n",
    "    def train(self, X, y, n_epochs=10, lr=0.001, n_classes=None):\n",
    "        self.lr = lr\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.classes = n_classes\n",
    "        if n_classes is None:\n",
    "            self.classes = set(y)\n",
    "            self.n_classes = len(self.classes)\n",
    "        \n",
    "        y_one_hot = one_hot_encode(y, self.n_classes)\n",
    "        self._init_neural_network()\n",
    "        \n",
    "        self.loss_e = 0\n",
    "        for e in range(n_epochs):\n",
    "            \n",
    "            if self.shuffle:\n",
    "                X, y_one_hot = shuffle_data(X, y_one_hot)\n",
    "                \n",
    "            for X_batch, y_batch in get_batch(X, y_one_hot, self.batch_size):\n",
    "                self._feed_forward(X_batch)\n",
    "                self._back_prop(X_batch, y_batch)\n",
    "                self.loss_batch = self.loss_func(y_batch, self.activations[-1])\n",
    "                self.loss_e += self.loss_batch\n",
    "        if self.verbose:\n",
    "            print(e, 'trn loss = {}'.format(self.loss_e))\n",
    "        print('epoch {}: final trn loss = {}'.format(e, self.loss_e))\n",
    "            \n",
    "    \n",
    "    def _feed_forward(self, X):\n",
    "        self.activations = []\n",
    "        self.Z_list = []\n",
    "        act = self.act\n",
    "        for layer, (w_l, b_l) in enumerate(zip(self.weights, self.biases)):\n",
    "            if layer == 0:\n",
    "                prev = X\n",
    "            else:\n",
    "                prev = self.activations[-1]\n",
    "\n",
    "            if layer == len(self.hidden):\n",
    "                act = self.last_act\n",
    "            Z_l = np.dot(prev, w_l) + b_l\n",
    "            act_l = act(Z_l)    \n",
    "            self.activations.append(act_l)\n",
    "            self.Z_list.append(Z_l)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        self._feed_forward(X)\n",
    "        return self.activations[-1]\n",
    "            \n",
    "    def _dE_dZ(self, y, p):\n",
    "        # dE/dz where E(y) - cross entropy and a(z) is the softmax activation function\n",
    "        return p - y\n",
    "    \n",
    "    def _get_grad(dE_da, da_dz):\n",
    "        return np.tensordot(dE_da, da_dz, axes=([-1],[0]))\n",
    "            \n",
    "    def _back_prop(self, X, y):\n",
    "        y_pred = self.activations[-1]\n",
    "        z_last = self.Z_list[-1]\n",
    "        self.dE_da = self.loss_grad_func(y, y_pred)\n",
    "        self.da_dz = self.last_act_grad(z_last) # gradient of last activation layer\n",
    "        self.dE_dz = get_dE_dz(self.dE_da, self.da_dz) # gradient at the last layer\n",
    "        self.dE_dz_1 = self.dE_dz[...]\n",
    "        self.dE_dz_2 = self._dE_dZ(y, y_pred)[...]\n",
    "\n",
    "        new_weights, new_biases = [], []\n",
    "        L = len(self.activations)\n",
    "        for layer in range(L-1, -1, -1):\n",
    "            w_l, b_l = self.weights[layer], self.biases[layer]\n",
    "            Z_l = self.Z_list[layer]\n",
    "            \n",
    "            if layer == 0:\n",
    "                act_prev = X\n",
    "            else:\n",
    "                act_prev = self.activations[layer-1]\n",
    "            \n",
    "            if layer < L-1:\n",
    "                #self.dE_dz = self._dE_dZ(y, y_pred)\n",
    "                dE_da = self.dE_dz @ self.weights[layer+1].T # dE_da wrt activation of current layer\n",
    "                da_dz = self.act_derivative(Z_l)\n",
    "                self.dE_dz = np.multiply(da_dz, dE_da)\n",
    "            \n",
    "            dE_dW = act_prev.T @ self.dE_dz\n",
    "            dE_db = np.sum(self.dE_dz, axis=0)\n",
    "            #print(layer, act_prev.T.shape, self.dE_dz.shape, dE_dW.shape, w_l.shape)\n",
    "            w_l -= self.lr * dE_dW\n",
    "            b_l -= self.lr * dE_db\n",
    "            \n",
    "            new_weights.append(w_l)\n",
    "            new_biases.append(b_l)\n",
    "            \n",
    "        self.weights = new_weights[::-1]\n",
    "        self.biases = new_biases[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trn.shape = (100, 2), X_test.shape = (50, 2)\n",
      "epoch 99: final trn loss = 36.444240410751206\n",
      "trn acc 0.97\n",
      "test acc 0.94\n"
     ]
    }
   ],
   "source": [
    "# Load Iris Dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  \n",
    "y = iris.target\n",
    "\n",
    "\n",
    "feature_idxs = [1, 3] # SET FEATURES BY INDEX <------------------\n",
    "\n",
    "feature_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "xlbl, ylbl = feature_names[feature_idxs[0]], feature_names[feature_idxs[1]] \n",
    "# We will also split the dataset into training and testing so we can evaluate the kNN classifier\n",
    "X_trn_, X_test_, y_trn, y_test = train_test_split(X, \n",
    "                                                 y, \n",
    "                                                 test_size=0.333, \n",
    "                                                 random_state=0,\n",
    "                                                 stratify=y)\n",
    "X_trn, X_test = X_trn_[:, feature_idxs], X_test_[:, feature_idxs]\n",
    "\n",
    "print(\"X_trn.shape = {}, X_test.shape = {}\".format(X_trn.shape, X_test.shape))\n",
    "X_trn_norm, (trn_mean, trn_std) = normalize_trn_data(X_trn)\n",
    "X_test_norm = (X_test - trn_mean) / trn_std\n",
    "\n",
    "\n",
    "nn = NeuralNetwork(hidden=(6,), \n",
    "                   init_weights='unit_norm', \n",
    "                   activation='ReLU',\n",
    "                   shuffle=True,\n",
    "                   batch_size=50,\n",
    "                   random_state=1\n",
    "                   )\n",
    "nn.train(X_trn_norm, y_trn, n_epochs=100, lr=0.01)\n",
    "y_pred_trn = nn.predict(X_trn_norm).argmax(axis=1)\n",
    "y_pred_test = nn.predict(X_test_norm).argmax(axis=1)\n",
    "\n",
    "print('trn acc', accuracy_score(y_pred_trn, y_trn))\n",
    "print('test acc', accuracy_score(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 3, 2]), array([2, 4, 3]))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_gradient(z,sm=None):\n",
    "    if sm is None:\n",
    "        sm = softmax(z)\n",
    "    res = np.einsum('ij,ik->ijk',sm,-sm)\n",
    "    np.einsum('ijj->ij',res)[...] += sm\n",
    "    return res\n",
    "\n",
    "def dE_dz(y, z, sm=None):\n",
    "    if sm is None:\n",
    "        sm = softmax(z)\n",
    "    dE_da = cross_entropy_derivative(y,sm)\n",
    "    da_dz = da_dz_pp(z,sm)\n",
    "    return np.einsum('ij,ijk->ik', dE_da, da_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[2.3 1.3] 1 (30, 2) (30,)\n",
      "[3.2 2.3] 2 (30, 2) (30,)\n",
      "[2.7 1.9] 2 (30, 2) (30,)\n",
      "[3.8 2.2] 2 (10, 2) (10,)\n",
      "\n",
      "1\n",
      "[2.2 1. ] 1 (30, 2) (30,)\n",
      "[3.1 0.2] 0 (30, 2) (30,)\n",
      "[2.8 2.4] 2 (30, 2) (30,)\n",
      "[3.1 0.2] 0 (10, 2) (10,)\n",
      "\n",
      "2\n",
      "[2.6 1.2] 1 (30, 2) (30,)\n",
      "[3.3 2.1] 2 (30, 2) (30,)\n",
      "[3.2 1.8] 2 (30, 2) (30,)\n",
      "[4.4 0.4] 0 (10, 2) (10,)\n",
      "\n",
      "3\n",
      "[3.  1.8] 2 (30, 2) (30,)\n",
      "[2.9 1.3] 1 (30, 2) (30,)\n",
      "[3.2 0.2] 0 (30, 2) (30,)\n",
      "[2.9 1.3] 1 (10, 2) (10,)\n",
      "\n",
      "4\n",
      "[2.2 1.5] 2 (30, 2) (30,)\n",
      "[3.  0.3] 0 (30, 2) (30,)\n",
      "[3.  1.8] 2 (30, 2) (30,)\n",
      "[3.2 0.2] 0 (10, 2) (10,)\n",
      "\n",
      "5\n",
      "[3.  1.8] 2 (30, 2) (30,)\n",
      "[3.4 2.3] 2 (30, 2) (30,)\n",
      "[2.9 1.4] 1 (30, 2) (30,)\n",
      "[3.2 0.2] 0 (10, 2) (10,)\n",
      "\n",
      "6\n",
      "[3.4 0.4] 0 (30, 2) (30,)\n",
      "[2.9 1.3] 1 (30, 2) (30,)\n",
      "[3.  1.5] 1 (30, 2) (30,)\n",
      "[2.7 1.6] 1 (10, 2) (10,)\n",
      "\n",
      "7\n",
      "[3.  1.8] 2 (30, 2) (30,)\n",
      "[2.3 1.3] 1 (30, 2) (30,)\n",
      "[2.9 1.3] 1 (30, 2) (30,)\n",
      "[2.6 1.2] 1 (10, 2) (10,)\n",
      "\n",
      "8\n",
      "[2.9 1.3] 1 (30, 2) (30,)\n",
      "[2.8 1.8] 2 (30, 2) (30,)\n",
      "[2.7 1.9] 2 (30, 2) (30,)\n",
      "[2.5 1.5] 1 (10, 2) (10,)\n",
      "\n",
      "9\n",
      "[3.  0.2] 0 (30, 2) (30,)\n",
      "[3.3 2.1] 2 (30, 2) (30,)\n",
      "[3.2 0.2] 0 (30, 2) (30,)\n",
      "[2. 1.] 1 (10, 2) (10,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(10):\n",
    "    XX, yy = shuffle_data(X_trn, y_trn)\n",
    "    print(e)\n",
    "    for xb, yb in get_batch(XX, yy, 30):\n",
    "        print(xb[0], yb[0], xb.shape, yb.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.36015288, -0.68509994,  0.32494705]),\n",
       " array([ 0.36015288, -0.68509994,  0.32494705]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.dE_dz_1[0], nn.dE_dz_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_norm_layer_init(input_shape, output_shape):\n",
    "    return np.random.normal(size=(input_shape, output_shape))\n",
    "\n",
    "def ones_layer_init(input_shape, output_shape):\n",
    "    return np.ones((input_shape, output_shape))\n",
    "\n",
    "def zeros_init_layer(length):\n",
    "    return np.zeros(length)\n",
    "\n",
    "def softmax(x):\n",
    "    # https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "    exp = np.exp(x - np.max(x))\n",
    "    return exp / np.sum(exp, axis=1)[:, None]\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(X):\n",
    "    return np.ones((X.shape[0]))\n",
    "\n",
    "def softmax_grad(x):\n",
    "    s = sigmoid(x)\n",
    "    _, K = s.shape\n",
    "    return s @ (np.eye(K, K) - s).T\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(x, 0, x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x > 0).astype(int)\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "    return -np.sum(y_true*np.log(y_pred)) / N\n",
    "\n",
    "def cross_entropy_derivative(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "    return -(y_true/y_pred) / N\n",
    "\n",
    "def onehot_encode(y):\n",
    "    y = np.array(y)\n",
    "    y_onehot = np.zeros((len(y), max(y)+1))\n",
    "    for i, y_i in enumerate(y):\n",
    "        y_onehot[i, y_i] = 1\n",
    "    return y_onehot\n",
    "\n",
    "def one_hot_encode(y, n_classes):\n",
    "    y_onehot = np.zeros((len(y), n_classes))\n",
    "    for i, y_i in enumerate(y):\n",
    "        y_onehot[i, y_i] = 1\n",
    "    return y_onehot\n",
    "'''\n",
    "def softmax_grad(x):\n",
    "    # da / dZ\n",
    "    # Reshape the 1-d softmax to 2-d so that np.dot will do the matrix multiplication\n",
    "    s = softmax(x).reshape(-1,1)\n",
    "    return np.diagflat(s) - np.dot(s, s.T)\n",
    "'''\n",
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hidden=(8, 6),\n",
    "                 init_weights='unit_norm',\n",
    "                 init_bias='zeros',\n",
    "                 activation='sigmoid',\n",
    "                 loss='cross_entropy',\n",
    "                 mode='classification',\n",
    "                 random_state=1):\n",
    "        self.hidden = hidden\n",
    "        self.init_weights = init_weights\n",
    "        self.init_bias = init_bias\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.random_state = random_state\n",
    "        self.mode = mode\n",
    "        np.random.seed(self.random_state)\n",
    "        self._set_act_func()\n",
    "        self._set_loss()\n",
    "        \n",
    "        \n",
    "    def _init_neural_network(self):\n",
    "        implemented_weight_inits = {'unit_norm': unit_norm_layer_init,\n",
    "                                    'ones': ones_layer_init\n",
    "                                   }\n",
    "        implemented_bias_inits = {'zeros': zeros_init_layer,\n",
    "                                   }\n",
    "        try:\n",
    "            init_layer_weight = implemented_weight_inits[self.init_weights]\n",
    "            init_layer_bias = implemented_bias_inits[self.init_bias]\n",
    "        except KeyError:\n",
    "            raise Exception('{} or {} not accepted'.format(self.init_weights,\n",
    "                                                           self.init_bias))\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for layer in range(len(self.hidden) + 1):\n",
    "            if layer == 0:\n",
    "                input_shape = self.n_features\n",
    "                output_shape = self.hidden[layer]\n",
    "            elif layer == len(self.hidden):\n",
    "                input_shape = self.hidden[layer - 1]\n",
    "                output_shape = self.n_classes\n",
    "            else:\n",
    "                input_shape = self.hidden[layer - 1]\n",
    "                output_shape = self.hidden[layer]                \n",
    "            w_l = init_layer_weight(input_shape, output_shape)\n",
    "            b_l = init_layer_bias(output_shape)\n",
    "            self.weights.append(w_l)\n",
    "            self.biases.append(b_l)\n",
    "        \n",
    "            \n",
    "    def _set_act_func(self):\n",
    "        implemented_activations = {'sigmoid': sigmoid,\n",
    "                                   'ReLU': ReLU,\n",
    "                                   'linear': linear_derivative}\n",
    "        # set activation function\n",
    "        try:\n",
    "            self.act = implemented_activations[self.activation]\n",
    "        except KeyError:\n",
    "            raise Exception('{} not accepted'.format(self.activation))\n",
    "            \n",
    "        implemented_derivatives = {'sigmoid': sigmoid_derivative,\n",
    "                                   'ReLU': ReLU_derivative,\n",
    "                                   'linear': linear_derivative}\n",
    "        \n",
    "        # set activation derivative (da/dz)\n",
    "        try:\n",
    "            self.act_derivative = implemented_derivatives[self.activation]\n",
    "        except KeyError:\n",
    "            raise Exception('derivative not implemented for {}'.format(self.activation))\n",
    "\n",
    "        # set activation for last layer (softmax for classification and linear for regression)\n",
    "        if self.mode == 'classification':\n",
    "            self.last_act = softmax\n",
    "        elif self.mode == 'regression':\n",
    "            self.last_act = linear\n",
    "\n",
    "    def _set_loss(self):\n",
    "        implemented_losses = {'cross_entropy': cross_entropy,}\n",
    "        try:\n",
    "            self.loss_func = implemented_losses[self.loss]\n",
    "        except KeyError:\n",
    "            raise Exception('{} not accepted'.format(self.loss))\n",
    "    \n",
    "    def train(self, X, y, n_epochs=10, lr=0.001, n_classes=None):\n",
    "        self.lr = lr\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.classes = n_classes\n",
    "        if n_classes is None:\n",
    "            self.classes = set(y)\n",
    "            self.n_classes = len(self.classes)\n",
    "        \n",
    "        y_one_hot = one_hot_encode(y, self.n_classes)\n",
    "        self._init_neural_network()\n",
    "        \n",
    "        for e in range(n_epochs):\n",
    "            # implement shuffle\n",
    "            # implement batch\n",
    "            self._feed_forward(X)\n",
    "            self.loss_e = self.loss_func(y_one_hot, self.activations[-1])\n",
    "            print('loss', e, self.loss_e)\n",
    "            self._back_prop(X, y_one_hot)\n",
    "\n",
    "    \n",
    "    def _feed_forward(self, X):\n",
    "        self.activations = []\n",
    "        self.Z_list = []\n",
    "        #self.Z_list.append(X)\n",
    "        act = self.act\n",
    "        for layer, (w_l, b_l) in enumerate(zip(self.weights, self.biases)):\n",
    "            if layer == 0:\n",
    "                prev = X\n",
    "            else:\n",
    "                prev = self.activations[-1]\n",
    "\n",
    "            if layer == len(self.hidden):\n",
    "                act = self.last_act\n",
    "            Z_l = np.dot(prev, w_l) + b_l\n",
    "            act_l = act(Z_l)    \n",
    "            self.activations.append(act_l)\n",
    "            self.Z_list.append(Z_l)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        self._feed_forward(X)\n",
    "        return self.activations[-1]\n",
    "            \n",
    "    def _dE_dZ(self, y, p):\n",
    "        # dE/dz where E(y) - cross entropy and a(z) is the softmax activation function\n",
    "        return p - y\n",
    "            \n",
    "    def _back_prop(self, X, y):\n",
    "        y_pred = self.activations[-1]\n",
    "        z_last = self.Z_list[-1]\n",
    "        \n",
    "        da_dz = self.last_act_grad(z_last) # gradient of last activation layer\n",
    "        # self.dE_dp = cross_entropy_derivative(y, y_pred)\n",
    "        new_weights, new_biases = [], []\n",
    "        L = len(self.activations)\n",
    "        for layer in range(L-1, -1, -1):\n",
    "            w_l, b_l = self.weights[layer], self.biases[layer]\n",
    "            Z_l = self.Z_list[layer]\n",
    "            \n",
    "            if layer == 0:\n",
    "                act_prev = X\n",
    "            else:\n",
    "                act_prev = self.activations[layer-1]\n",
    "            \n",
    "            if layer == L-1:\n",
    "                self.dE_dz = self._dE_dZ(y, y_pred)\n",
    "            else:\n",
    "                dE_da = self.dE_dz @ self.weights[layer+1].T # dE_da wrt activation of current layer\n",
    "                da_dz = self.act_derivative(Z_l)\n",
    "                self.dE_dz = np.multiply(da_dz, dE_da)\n",
    "            \n",
    "            dE_dW = act_prev.T @ self.dE_dz\n",
    "            dE_db = np.sum(self.dE_dz, axis=0)\n",
    "            print(layer, act_prev.T.shape, self.dE_dz.shape, dE_dW.shape, w_l.shape)\n",
    "            w_l -= self.lr * dE_dW\n",
    "            b_l -= self.lr * dE_db\n",
    "            \n",
    "            new_weights.append(w_l)\n",
    "            new_biases.append(b_l)\n",
    "            \n",
    "        self.weights = new_weights[::-1]\n",
    "        self.biases = new_biases[::-1]\n",
    "            #Z_l = self.Z_list[layer]\n",
    "            #self.act_l = self.activations[layer]\n",
    "            #prev_act = self.activations[layer-1]\n",
    "            \n",
    "            \n",
    "            #self.da_dz = softmax_grad(Z_l)\n",
    "            #if layer == 0: # last layer\n",
    "            #    dE_dZ_l = dE_dyp # N vec X N vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris Dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  \n",
    "y = iris.target\n",
    "\n",
    "# For illustration purposes we will only be using the two features in the dataset\n",
    "feature_idxs = [1, 3] # SET FEATURES BY INDEX <------------------\n",
    "\n",
    "feature_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "xlbl, ylbl = feature_names[feature_idxs[0]], feature_names[feature_idxs[1]] \n",
    "# We will also split the dataset into training and testing so we can evaluate the kNN classifier\n",
    "X_trn_, X_test_, y_trn, y_test = train_test_split(X, \n",
    "                                                 y, \n",
    "                                                 test_size=0.333, \n",
    "                                                 random_state=0,\n",
    "                                                 stratify=y)\n",
    "X_trn, X_test = X_trn_[:, feature_idxs], X_test_[:, feature_idxs]\n",
    "\n",
    "print(\"X_trn.shape = {}, X_test.shape = {}\".format(X_trn.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "nn = NeuralNetwork(hidden=(8,6), \n",
    "                   init_weights='unit_norm', \n",
    "                   activation='ReLU',\n",
    "                   )\n",
    "nn.train(X_trn, y_trn, n_epochs=100, lr=0.0001)\n",
    "y_pred_trn = nn.predict(X_trn).argmax(axis=1)\n",
    "y_pred_test = nn.predict(X_test).argmax(axis=1)\n",
    "print(accuracy_score(y_pred_trn, y_trn))\n",
    "print(accuracy_score(y_pred_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(100, 2)\n",
    "y = np.random.randint(0, 2, size=100)\n",
    "\n",
    "nn = NeuralNetwork(hidden=(8,6), \n",
    "                   init_weights='unit_norm', \n",
    "                   activation='ReLU',\n",
    "                   )\n",
    "nn.train(X, y, n_epochs=100, lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(100, 2)\n",
    "y = np.random.randint(0, 2, size=100)\n",
    "\n",
    "nn = NeuralNetwork(hidden=(8,6), init_weights='unit_norm', activation='ReLU')\n",
    "nn.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = [[1, 1], [0, 0], [0, 1]]\n",
    "y = [1, 0, 1]\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "nn = NeuralNetwork(hidden=(8,6), init_weights='unit_norm', activation='ReLU')\n",
    "nn.train(X, y)\n",
    "print(nn.loss_e)\n",
    "for i in range(len(nn.activations)):\n",
    "    \n",
    "    print(i, nn.weights[i], '-', nn.biases[i],'-', nn.activations[i])\n",
    "    \n",
    "for i in range(len(nn.activations)):\n",
    "    print(nn.weights[i].shape, '-', nn.biases[i].shape,'-', nn.activations[i].shape, '-', nn.Z_list[i].shape)\n",
    "\n",
    "print(nn.dE_dz.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_derivative(y_true, y_pred):\n",
    "    # dE / da\n",
    "    # input: N x K\n",
    "    # output: N x K array\n",
    "    N = len(y_true)\n",
    "    return -(y_true / y_pred) / N\n",
    "\n",
    "def softmax(x):\n",
    "    # activation (a)\n",
    "    # input: N x K array\n",
    "    # output: N x K array\n",
    "    # https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "    exp = np.exp(x - np.max(x))\n",
    "    return exp / np.sum(exp, axis=1)[:, None]\n",
    "\n",
    "def softmax_derivative(Z):\n",
    "    # da/dz\n",
    "    #input: N x K array\n",
    "    #output: N x K x K array\n",
    "    #http://saitcelebi.com/tut/output/part2.html\n",
    "    s = softmax(Z)[:, :, np.newaxis]\n",
    "    a = np.tensordot(s, np.ones((1, K)), axes=([-1],[0]))\n",
    "    I = np.repeat(np.eye(K, K)[np.newaxis, :, :], N, axis=0)\n",
    "    b = I - np.tensordot(np.ones((K, 1)), s.T, axes=([-1],[0])).T\n",
    "    return a * np.swapaxes(b, 1, 2)\n",
    "\n",
    "def softmax_derivative_test(Z):\n",
    "    # da/dz\n",
    "    # non-vectorized softmax gradient calculation\n",
    "    #http://saitcelebi.com/tut/output/part2.html\n",
    "    N, K = Z.shape\n",
    "    da_dz = np.zeros((N, K, K))\n",
    "    kron_delta = np.eye(K)\n",
    "    s = softmax(Z)\n",
    "    for n in range(N):\n",
    "        for i in range(K):\n",
    "            for j in range(K):\n",
    "                da_dz[n, i, j] = s[n, i] * (kron_delta[i, j] - s[n, j])\n",
    "    return da_dz\n",
    "\n",
    "\n",
    "def dE_dz_test2(dE_da, da_dz):\n",
    "    # array (N x K)\n",
    "    # array (N x K x K)\n",
    "    # output: array (N x K)\n",
    "    N, K = dE_da.shape\n",
    "    dE_dz = np.zeros((N, K))\n",
    "    for n in range(N):\n",
    "        dE_dz[n, :] = np.matmul(da_dz[n], dE_da[n, :, np.newaxis]).T\n",
    "    return dE_dz\n",
    "        \n",
    "def some_type_of_matrix_multiplication_(dE_da, da_dz):\n",
    "    # how do i get dE/dz from dE_da and da_dz\n",
    "    pass\n",
    "\n",
    "X = np.random.rand(100, 2)\n",
    "W = np.random.rand(2, 4)\n",
    "y = np.random.randint(0, 4, size=100)\n",
    "y = one_hot_encode(y, 4)\n",
    "Z = X @ W\n",
    "S = softmax(Z)\n",
    "N, K = Z.shape\n",
    "\n",
    "# da / dz for softmax\n",
    "da_dz = softmax_derivative(Z) # (100, 4, 4)\n",
    "da_dz_test = softmax_derivative_test(Z) # (100, 4, 4) - non vectorized implementation\n",
    "print(np.isclose(da_dz, da_dz_test).all()) # equivalence test\n",
    "\n",
    "dE_da = cross_entropy_derivative(y, S) # (100, 4)\n",
    "dE_dz = some_type_of_matrix_multiplication_(dE_da, da_dz) # what do I do here? *****\n",
    "dE_dz_test  = (S - y) / N # (100, 4) If you combine dE/da and da/dz terms\n",
    "dE_dz_test2 = dE_dz_test2(dE_da, da_dz)\n",
    "print(np.isclose(dE_dz_test, dE_dz_test2).all()) # equivalence test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_dz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dE_dz_test2(dE_da, da_dz):\n",
    "    N, K = dE_da.shape\n",
    "    dE_dz = np.zeros((N, K))\n",
    "    for n in range(N):\n",
    "        dE_dz[n, :] = np.matmul(da_dz[n], dE_da[n, :, np.newaxis]).T\n",
    "    return dE_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_dz.shape, dE_da.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_dz_test2_ = dE_dz_test2(dE_da, da_dz)\n",
    "dE_dz_test[0], dE_dz_test2_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_dz_test_2 =  np.matmul(da_dz[0], dE_da[0, :, np.newaxis]).T\n",
    "dE_dz_test[0], dE_dz_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.tensordot(da_dz, dE_da[:, np.newaxis, :], axes=([1, 2],[1, 2]),  ).shape\n",
    "\n",
    "#dz = np.matmul(matrix, da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_dz.shape, dE_da[:, np.newaxis, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_da[:, :, np.newaxis].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_da.shape, da_dz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_derivative_test(Z)[0], softmax_derivative(Z)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(100, 2)\n",
    "W = np.random.rand(2, 4)\n",
    "y = np.random.randint(0, 4, size=100)\n",
    "y = one_hot_encode(y, 4)\n",
    "Z = X @ W\n",
    "N, K = Z.shape\n",
    "s = softmax(Z)[:, :, np.newaxis]\n",
    "dS = np.zeros((N, K, K))\n",
    "#for n in range(N):\n",
    "#    ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = np.tensordot(s, np.ones((1, K)), axes=([-1],[0])) # np.matmul(s[:, :, np.newaxis], np.ones((1, K)))\n",
    "I = np.repeat(np.eye(K, K)[np.newaxis, :, :], N, axis=0)\n",
    "b = I - np.tensordot(np.ones((K, 1)), s.T, axes=([-1],[0])).T\n",
    "c = a * np.swapaxes(b, 1, 2)\n",
    "#np.matmul(np.ones((K, 1)), s[:, :, np.newaxis].T)\n",
    "#b = np.identity(K) - a.T\n",
    "#  matrix = np.matmul(a, np.ones((1, 3))) * (np.identity(3) - np.matmul(np.ones((3, 1)), a.T))\n",
    "# dz = np.matmul(matrix, da)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://saitcelebi.com/tut/output/part2.html\n",
    "a = np.tensordot(s[:, :, np.newaxis], np.ones((1, K)), axes=([-1],[0])) # np.matmul(s[:, :, np.newaxis], np.ones((1, K)))\n",
    "I = np.repeat(np.eye(K, K)[np.newaxis, :, :], N, axis=0)\n",
    "b = I - np.tensordot(np.ones((K, 1)), s[:, :, np.newaxis].T, axes=([-1],[0])).T\n",
    "c = a * np.swapaxes(b, 1, 2)\n",
    "#np.matmul(np.ones((K, 1)), s[:, :, np.newaxis].T)\n",
    "#b = np.identity(K) - a.T\n",
    "#  matrix = np.matmul(a, np.ones((1, 3))) * (np.identity(3) - np.matmul(np.ones((3, 1)), a.T))\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[0], tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0], tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = s[0]\n",
    "I = np.eye(K)\n",
    "tmp = I - np.matmul(np.ones((K, 1)), s2.T)\n",
    "tmp2 = np.matmul(s2, np.ones((1, K)))\n",
    "tmp3 = tmp2 * tmp\n",
    "tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(np.ones(K), s2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones(K), s2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = np.sum(nn.dE_dz, axis=0)\n",
    "np.ones(nn.dE_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.random.rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, K = s.shape\n",
    "I = np.repeat(np.eye(K, K)[np.newaxis, :, :], N, axis=0)\n",
    "(I - s @ np.eye(K, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.multiply.outer(s, np.eye(1, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = s[0, : , np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://saitcelebi.com/tut/output/part2.html\n",
    "\n",
    "s0 = s[0, : , np.newaxis]\n",
    "tmp = np.identity(K) - np.matmul(np.ones((K, 1)), s0.T)\n",
    "tmp2 = np.matmul(s0, np.ones((1, K)))\n",
    "tmp3 = tmp2 * tmp\n",
    "tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(np.ones((K, 1)), s0.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones((K, 1)).shape, s0.T.shape, np.matmul(np.ones((K, 1)), s0.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I2 = np.ones((N, K))[:, : , np.newaxis]\n",
    "\n",
    "print(I2.shape, s2.T.shape), \n",
    "print(np.tensordot(I2, s2.T, axes=([2],[0])).shape) #np.matmul(I2, s2.T).shape np.tensordot(I2, s2.T, axes=([2,1],[0,1])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(I, s2.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.eye(2, 2)\n",
    "print(a.shape)\n",
    "# (2,  2)\n",
    "\n",
    "# indexing with np.newaxis inserts a new 3rd dimension, which we then repeat the\n",
    "# array along, (you can achieve the same effect by indexing with None, see below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0, 1, 2, 1, 0]\n",
    "y_ = np.zeros( (len(y), len(set(y))))\n",
    "y_[:, y[:]] = 1\n",
    "y_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
