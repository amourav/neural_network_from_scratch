{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network From Scratch\n",
    "## classification demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from andreiNet.neural_net import NeuralNetwork\n",
    "from andreiNet.utils import norm_data, one_hot_encode\n",
    "from andreiNet.metrics import accuracy\n",
    "from andreiNet.losses import Loss, CrossEntropy, FocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Cross Entropy = - \\frac{1}{N} \\sum_{k} \\sum_{i} y_{i, k} \\log(p_{i, k}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Focal Loss = - \\frac{1}{N} \\sum_{k} \\sum_{i} \\alpha_t (1 - p^t_{i, k})^{\\gamma} y_{i, k} \\log(p^t_{i, k}) $$\n",
    "$$ p^t_{i, k} = y_{i, k} \\times p_{i, k} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy: 0.9830564281864164\n",
      "focal loss (gamma=0.0): 0.9830564281864164\n",
      "focal loss (gamma=2.0): 0.5310705044561551\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([[1, 0, 0], [0, 1, 0]])\n",
    "y_pred = np.array([[0.7, 0.2, 0.1], [0.2, 0.2, 0.6]])\n",
    "\n",
    "ce = CrossEntropy().loss(y_true, y_pred)\n",
    "print(\"cross entropy: {}\".format(ce))\n",
    "\n",
    "gamma = 0.0\n",
    "fl = FocalLoss(gamma=gamma).loss(y_true, y_pred)\n",
    "print(\"focal loss (gamma={}): {}\".format(gamma, fl))\n",
    "\n",
    "gamma = 2.0\n",
    "fl = FocalLoss(gamma=gamma).loss(y_true, y_pred)\n",
    "print(\"focal loss (gamma={}): {}\".format(gamma, fl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy: 0.10536051565782628\n",
      "focal loss (gamma=2.0): 0.0010536051565782623\n",
      "ce / fl = 100.00000000000006\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Easy Case, Correctly Classified\n",
    "y_true = np.array([[1, 0, 0], ])\n",
    "y_pred = np.array([[0.9, 0.05, 0.05], ])\n",
    "\n",
    "ce = CrossEntropy().loss(y_true, y_pred)\n",
    "print(\"cross entropy: {}\".format(ce))\n",
    "\n",
    "fl = FocalLoss(gamma=gamma).loss(y_true, y_pred)\n",
    "print(\"focal loss (gamma={}): {}\".format(gamma, fl))\n",
    "\n",
    "print('ce / fl = {}'.format(ce / fl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy: 0.916290731874155\n",
      "focal loss (gamma=2.0): 0.3298646634746958\n",
      "ce / fl = 2.7777777777777777\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Hard Case, Correctly Classified\n",
    "y_true = np.array([[1, 0, 0], ])\n",
    "y_pred = np.array([[0.4, 0.3, 0.3], ])\n",
    "\n",
    "ce = CrossEntropy().loss(y_true, y_pred)\n",
    "print(\"cross entropy: {}\".format(ce))\n",
    "\n",
    "fl = FocalLoss(gamma=gamma).loss(y_true, y_pred)\n",
    "print(\"focal loss (gamma={}): {}\".format(gamma, fl))\n",
    "\n",
    "print('ce / fl = {}'.format(ce / fl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy: 1.2039728043259361\n",
      "focal loss (gamma=2.0): 0.5899466741197086\n",
      "ce / fl = 2.0408163265306127\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Incorrectly Classified\n",
    "y_true = np.array([[1, 0, 0], ])\n",
    "y_pred = np.array([[0.3, 0.4, 0.4], ])\n",
    "\n",
    "ce = CrossEntropy().loss(y_true, y_pred)\n",
    "print(\"cross entropy: {}\".format(ce))\n",
    "\n",
    "fl = FocalLoss(gamma=gamma).loss(y_true, y_pred)\n",
    "print(\"focal loss (gamma={}): {}\".format(gamma, fl))\n",
    "\n",
    "print('ce / fl = {}'.format(ce / fl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy: 2.995732273553991\n",
      "focal loss (gamma=2.0): 2.7036483768824766\n",
      "ce / fl = 1.10803324099723\n"
     ]
    }
   ],
   "source": [
    "# Example 4: (very) Incorrectly Classified\n",
    "y_true = np.array([[1, 0, 0], ])\n",
    "y_pred = np.array([[0.05, 0.9, 0.05], ])\n",
    "\n",
    "ce = CrossEntropy().loss(y_true, y_pred)\n",
    "print(\"cross entropy: {}\".format(ce))\n",
    "\n",
    "fl = FocalLoss(gamma=gamma).loss(y_true, y_pred)\n",
    "print(\"focal loss (gamma={}): {}\".format(gamma, fl))\n",
    "\n",
    "print('ce / fl = {}'.format(ce / fl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-12.35810868,   0.        ,  -0.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CrossEntropy().grad(y_true, y_pred)\n",
    "FocalLoss2(gamma=2.0).grad(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trn.shape = (75, 4), X_test.shape = (75, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load Iris Dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  \n",
    "y = iris.target\n",
    "\n",
    "# We will also split the dataset into training and testing so we can evaluate the kNN classifier\n",
    "X_trn, X_test, y_trn, y_test = train_test_split(X, \n",
    "                                                y, \n",
    "                                                test_size=0.50, \n",
    "                                                random_state=0,\n",
    "                                                stratify=y)\n",
    "\n",
    "print(\"X_trn.shape = {}, X_test.shape = {}\".format(X_trn.shape, X_test.shape))\n",
    "X_trn_norm, (trn_mean, trn_std) = norm_data(X_trn)\n",
    "X_test_norm = (X_test - trn_mean) / trn_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "activation = 'relu'\n",
    "batch_size = 50\n",
    "random_state = 0\n",
    "lr = 0.001\n",
    "n_epochs = 1000\n",
    "metrics = ['accuracy']\n",
    "weight_init = 'he_norm'\n",
    "hidden_layers = (50, 60, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "alpha (1.0) not accepted",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-b78936f78e7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m          \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m          \u001b[0mval_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m          save_best=True)\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Run Inference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\projects\\neural_network_from_scratch\\andreiNet\\neural_net.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y, val_data, n_epochs, lr, batch_size, n_classes, save_best, early_stop)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_back_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_one_hot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_one_hot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_best_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\projects\\neural_network_from_scratch\\andreiNet\\neural_net.py\u001b[0m in \u001b[0;36m_update_metrics\u001b[1;34m(self, X, y_ohe, val_data)\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[0my_val_ohe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_ohe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\projects\\neural_network_from_scratch\\andreiNet\\neural_net.py\u001b[0m in \u001b[0;36m_get_metrics\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLoss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m                 \u001b[0mmetric_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             \u001b[0mmetric_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetric_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmetric_vals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\projects\\neural_network_from_scratch\\andreiNet\\losses.py\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \"\"\"\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[1;31m# down weight easy examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\projects\\neural_network_from_scratch\\andreiNet\\losses.py\u001b[0m in \u001b[0;36m_set_alpha\u001b[1;34m(self, y_true)\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'alpha ({}) not accepted'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: alpha (1.0) not accepted"
     ]
    }
   ],
   "source": [
    "loss = CrossEntropy() #'cross_entropy'\n",
    "\n",
    "\n",
    "# Train model\n",
    "\n",
    "# Initialize model\n",
    "start_time = time.time()\n",
    "nn = NeuralNetwork(hidden=hidden_layers, \n",
    "                   init_weights=weight_init,\n",
    "                   loss=loss,\n",
    "                   activation=activation,\n",
    "                   shuffle=True,\n",
    "                   random_state=random_state,\n",
    "                   metrics=metrics,\n",
    "                   verbose=False\n",
    "                   )\n",
    "\n",
    "nn.train(X_trn_norm, y_trn, \n",
    "         n_epochs=n_epochs,\n",
    "         batch_size=batch_size, \n",
    "         early_stop=None, # ('accuracy', 500),\n",
    "         lr=lr, \n",
    "         val_data=(X_test_norm, y_test),\n",
    "         save_best=True)\n",
    "\n",
    "# Run Inference\n",
    "y_pred_trn = nn.predict(X_trn_norm).argmax(axis=1)\n",
    "y_pred_test = nn.predict(X_test_norm).argmax(axis=1)\n",
    "\n",
    "delta_1 = time.time() - start_time\n",
    "print(\"--- %s seconds ---\" % (delta_1))\n",
    "print('trn acc', accuracy(y_pred_trn, y_trn))\n",
    "print('test acc', accuracy(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-6632def3e0b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m          \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m          \u001b[0mval_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m          save_best=True)\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Run Inference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\projects\\neural_network_from_scratch\\andreiNet\\neural_net.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y, val_data, n_epochs, lr, batch_size, n_classes, save_best, early_stop)\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_one_hot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_back_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_one_hot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_one_hot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\projects\\neural_network_from_scratch\\andreiNet\\neural_net.py\u001b[0m in \u001b[0;36m_back_prop\u001b[1;34m(self, X, y, lr)\u001b[0m\n\u001b[0;32m    407\u001b[0m         self.dL_dz = self._get_gradient(y=y,\n\u001b[0;32m    408\u001b[0m                                         \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m                                         z=self.z_list[-1])\n\u001b[0m\u001b[0;32m    410\u001b[0m         \u001b[0mnew_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_biases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\projects\\neural_network_from_scratch\\andreiNet\\neural_net.py\u001b[0m in \u001b[0;36m_get_gradient\u001b[1;34m(self, y, a, z)\u001b[0m\n\u001b[0;32m    393\u001b[0m         \"\"\"\n\u001b[0;32m    394\u001b[0m         \u001b[1;31m# https://stackoverflow.com/questions/57741998/vectorizing-softmax-cross-entropy-gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m         \u001b[0mdL_da\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_grad_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m         \u001b[0mda_dz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_act_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ij,ijk->ik'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdL_da\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mda_dz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\projects\\neural_network_from_scratch\\andreiNet\\losses.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtmp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtmp\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtmp2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtmp\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "loss = FocalLoss(gamma=2.0) #'cross_entropy'\n",
    "\n",
    "\n",
    "# Train model\n",
    "\n",
    "# Initialize model\n",
    "start_time = time.time()\n",
    "nn = NeuralNetwork(hidden=hidden_layers, \n",
    "                   init_weights=weight_init,\n",
    "                   loss=loss,\n",
    "                   activation=activation,\n",
    "                   shuffle=True,\n",
    "                   random_state=random_state,\n",
    "                   metrics=metrics,\n",
    "                   verbose=False\n",
    "                   )\n",
    "\n",
    "nn.train(X_trn_norm, y_trn, \n",
    "         n_epochs=n_epochs,\n",
    "         batch_size=batch_size, \n",
    "         early_stop=None, # ('accuracy', 500),\n",
    "         lr=lr, \n",
    "         val_data=(X_test_norm, y_test),\n",
    "         save_best=True)\n",
    "\n",
    "# Run Inference\n",
    "y_pred_trn = nn.predict(X_trn_norm).argmax(axis=1)\n",
    "y_pred_test = nn.predict(X_test_norm).argmax(axis=1)\n",
    "\n",
    "delta_1 = time.time() - start_time\n",
    "print(\"--- %s seconds ---\" % (delta_1))\n",
    "print('trn acc', accuracy(y_pred_trn, y_trn))\n",
    "print('test acc', accuracy(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trn_oh = one_hot_encode(y_trn, len(set(y_trn)))\n",
    "y_trn_pred_oh = nn.predict(X_trn_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = loss.loss(y_trn_oh, y_trn_pred_oh)\n",
    "ce_grad = loss.grad(y_trn_oh, y_trn_pred_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2420580872969582\n",
      "[-0.0134586 -0.        -0.       ]\n"
     ]
    }
   ],
   "source": [
    "print(ce)\n",
    "print(ce_grad[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = FocalLoss(gamma=0.0, alpha='inverse_freq')\n",
    "fl_loss = fl.loss(y_trn_oh, y_trn_pred_oh)\n",
    "fl_grad = fl.grad(y_trn_oh, y_trn_pred_oh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trn_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00968232349187833\n",
      "[-0.00053834 -0.         -0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(ce/25)\n",
    "print(ce_grad[0]/25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00968232349187833, array([-0.00053834, -0.        , -0.        ]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl_loss, fl_grad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot neural network history\n",
    "#loss = 'cross_entropy'\n",
    "fig_size = (12, 5)\n",
    "x_axis = np.arange(len(nn.trn_metric_hist[loss]))\n",
    "metrics = nn.trn_metric_hist.keys()\n",
    "fig, axs = plt.subplots(1, 2, figsize=fig_size)\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axs[i].plot(x_axis, nn.trn_metric_hist[metric], c='r', linewidth=2, label='trn')\n",
    "    axs[i].plot(x_axis, nn.val_metric_hist[metric], c='b', linewidth=2, label='val')\n",
    "    axs[i].set_xlabel('epoch', fontsize=18)\n",
    "    axs[i].set_ylabel(metric, fontsize=18)\n",
    "    if metric == loss:\n",
    "        axs[i].set_ylabel(str(metric)+' (loss)', \n",
    "                          fontsize=18)\n",
    "    axs[i].legend(prop={'size': 14})\n",
    "    \n",
    "plt.suptitle('andreiNet model history', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Keras model\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "np.random.seed(random_state)\n",
    "start_time = time.time()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "for l, hidden in enumerate(hidden_layers):\n",
    "    output_shape = hidden\n",
    "    if l==0:\n",
    "        input_shape = X_trn_norm.shape[1]\n",
    "    else:\n",
    "        input_shape = hidden_layers[l-1]\n",
    "    model.add(Dense(output_shape, activation=str(activation).lower(), \n",
    "                input_dim=input_shape, \n",
    "                kernel_initializer='he_normal'))\n",
    "    \n",
    "model.add(Dense(3, activation='softmax',))\n",
    "\n",
    "sgd = SGD(lr=lr, decay=0.0, momentum=0.00, nesterov=False)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "keras_hist = model.fit(X_trn_norm, one_hot_encode(y_trn, 3),\n",
    "                       epochs=n_epochs,\n",
    "                       validation_data=(X_test_norm, one_hot_encode(y_test, 3)),\n",
    "                       batch_size=batch_size,\n",
    "                       verbose=0)\n",
    "y_pred_trn = model.predict(X_trn_norm).argmax(axis=1)\n",
    "y_pred_test = model.predict(X_test_norm).argmax(axis=1)\n",
    "delta_2 = time.time() - start_time\n",
    "print(\"--- %s seconds ---\" % (delta_2))\n",
    "print('trn acc', accuracy(y_pred_trn, y_trn))\n",
    "print('test acc', accuracy(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras training history\n",
    "fig_size = (12, 5)\n",
    "x_axis = np.arange(n_epochs)\n",
    "fig, axs = plt.subplots(1, 2, figsize=fig_size)\n",
    "\n",
    "axs[0].plot(x_axis, keras_hist.history['acc'], c='r', linewidth=2, label='trn')\n",
    "axs[0].plot(x_axis, keras_hist.history['val_acc'], c='b', linewidth=2, label='val')\n",
    "axs[0].set_xlabel('epoch', fontsize=18)\n",
    "axs[0].set_ylabel('accuracy', fontsize=18)\n",
    "axs[0].legend(prop={'size': 14})\n",
    "\n",
    "axs[1].plot(x_axis, keras_hist.history['loss'], c='r', linewidth=2, label='trn')\n",
    "axs[1].plot(x_axis, keras_hist.history['val_loss'], c='b', linewidth=2, label='val')\n",
    "axs[1].set_xlabel('epoch', fontsize=18)\n",
    "axs[1].set_ylabel('categorical_crossentropy', fontsize=18)\n",
    "axs[1].legend(prop={'size': 14})\n",
    "\n",
    "plt.suptitle('Keras model history', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare training time\n",
    "plt.figure(figsize=(6, 6))\n",
    "trn_times = [delta_1, delta_2]\n",
    "y_pos = [0, 1]\n",
    "plt.bar(y_pos, trn_times, color=['blue', 'red'])\n",
    "plt.xticks(y_pos, ['andreiNet', 'Keras'], fontsize=16) \n",
    "plt.ylabel('training time (sec)', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
